{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This notebook is for the exploration of Logistic Regression -- it corresponds to Lecture Handout 2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload setup (you don't need to edit this cell); instructions to: \n",
    "#   i) enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "#  ii) import the module 'lab_1' (which will contain your functions) in an autoreloadable way \n",
    "%aimport lab_2\n",
    "# iii) indicate that we want autoreloading to happen on every evaluation.\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 1: import\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.7\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 2. loading the ISLR 'Default' dataset\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "# see https://cran.r-project.org/web/packages/ISLR/ISLR.pdf\n",
    "#\n",
    "# This data set contains information on ten thousand customers. \n",
    "#\n",
    "# The aim here is to predict which customers will default on their credit card debt.\n",
    "#\n",
    "# The dataset contains 10000 observations on the following 4 variables.\n",
    "#   * 'default': a No/Yes label indicating whether the customer defaulted on their debt\n",
    "#   * 'student': a No/Yes label indicating whether the customer is a student\n",
    "#   * 'balance': the average balance that the customer has remaining on their credit card after making\n",
    "#                their monthly payment\n",
    "#   * 'income' : income of customer\n",
    "\n",
    "df = pd.read_csv('Default.csv')\n",
    "\n",
    "# we are using here the pandas python package to read the CSV file.\n",
    "# we can look at the first 10 observations\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# 3. visualise your data\n",
    "#\n",
    "##############################################################\n",
    "\n",
    "# We need to visualise our data\n",
    "# let's ignore the categorical features 'student' and the first column\n",
    "# and focus on the 'balance' and 'income' features\n",
    "\n",
    "balance = df['balance'].values\n",
    "income = df['income'].values\n",
    "\n",
    "# the outcome is of boolean type\n",
    "y = df['default'].values == 'Yes'\n",
    "\n",
    "# setting up the matplotlib figure\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "# we are only going to plot a subset of the data\n",
    "income_subset = income[0:1000];\n",
    "balance_subset = balance[0:1000];\n",
    "y_subset = y[0:1000];\n",
    "\n",
    "# plotting balance vs income for the 'No Default' class\n",
    "ax.scatter(balance_subset[y_subset == False], income_subset[y_subset == False], s=15, marker='o')\n",
    "\n",
    "# plotting balance vs income for the 'Default' class\n",
    "ax.scatter(balance_subset[y_subset == True],  income_subset[y_subset == True],  s=40, marker='+')\n",
    "\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_ylabel('Income')\n",
    "ax.set_xlim(xmin=-100)\n",
    "ax.set_xlabel('Balance')\n",
    "ax.legend(['No Default', 'Default'])\n",
    "\n",
    "# after a quick look at the graph, it appears that \n",
    "# the most relevant feature is 'balance'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: implement cross-entropy (in the lab_2 module)\n",
    "\n",
    "w_test = np.array([0.1, 0.3])\n",
    "X_test = np.array([[1,1], [1,0]])\n",
    "y_test = np.array([0, 1])\n",
    "print(lab_2.cross_entropy(w_test, X_test, y_test))\n",
    "\n",
    "# Should print 0.778703757908 if your function is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: implement gradient computation (in the lab_2 module)\n",
    "w_test = np.array([0.1, 0.3])\n",
    "X_test = np.array([[1,1], [1,0]])\n",
    "y_test = np.array([0, 1])\n",
    "print(lab_2.gradient(w_test, X_test, y_test))\n",
    "# Expected result: [ 0.06183342  0.29934383]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient descent algorithm as explained in page 26\n",
    "# the learning_rate refers to the greek letter 'eta'\n",
    "# The method also returns the vector of loss function\n",
    "def gradient_descent(w0, X, y, learning_rate, maxit):\n",
    "    w = w0\n",
    "    loss = [];\n",
    "    for i in range(0, maxit):\n",
    "        if (i % 83 == 0):\n",
    "            loss_error = lab_2.cross_entropy(w, X, y)\n",
    "            loss.append(loss_error)\n",
    "      \n",
    "        grad = lab_2.gradient(w,X,y);\n",
    "        w = w - learning_rate * grad;        \n",
    "        i = i + 1\n",
    "        \n",
    "    return w, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Setting up the Model and Design Matrix\n",
    "#\n",
    "\n",
    "# in this lab we will look at the following model:\n",
    "# y = [ w0 + w1 * balance > 0]\n",
    "#\n",
    "# and ignore the 'income' feature\n",
    "\n",
    "n = balance.shape[0]\n",
    "p = 2\n",
    "\n",
    "X = np.zeros(shape=(n, p))\n",
    "\n",
    "# the first feature is 1 (with associated weight w0)\n",
    "X[:,0] = 1;\n",
    "\n",
    "# the second feature is the balance values (with associated weight w1)\n",
    "X[:,1] = balance[:]\n",
    "\n",
    "# Gradient Descent algorithms are quite sensitive to initial conditions\n",
    "# and it is often beneficial to rescale the features\n",
    "# to improve the performance. \n",
    "# in our case we note that balance ranges from 0 to about 2500.\n",
    "# this is much larger in magnitude to the first feature which is simply 1.\n",
    "# Thus we (arbitrarily) rescale by factor of 1/1000\n",
    "\n",
    "X[:,1] = X[:,1]/1000;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the following we are studying the convergence for 3 different learning rates\n",
    "\n",
    "w0 = [0, 0];\n",
    "w, loss = gradient_descent(w0, X, y, learning_rate=50, maxit=10000);\n",
    "print(w)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "ax.plot(loss)\n",
    "\n",
    "w0 = [0, 0];\n",
    "w, loss = gradient_descent(w0, X, y, learning_rate=30, maxit=10000);\n",
    "print(w)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "ax.plot(loss)\n",
    "\n",
    "w0 = [0, 0];\n",
    "w, loss = gradient_descent(w0, X, y, learning_rate=1, maxit=10000);\n",
    "print(w)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "ax.plot(loss)\n",
    "\n",
    "# Question 3\n",
    "# What is the best learning rate value out of 50, 30 and 1 ?\n",
    "# This cell might take a while to evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good way to check that if you reached convergence is to check if the gradient is null (or at least very small):\n",
    "lab_2.gradient(w, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# In logistic Regression, we set a parametric model for the likelihood \n",
    "# we denote logit = x'w and parametrise the likelihood as\n",
    "# p(y_i|logit) = 1/(1 + exp(-logit))\n",
    "#\n",
    "# We want to verify that this is a correct approximation for our problem\n",
    "#\n",
    "# The following function makes an empirical measurement of p(y_i|logit)\n",
    "# by recording in the dataset the proportion of default=True for \n",
    "# a particular logit value (within some small threshold T).\n",
    "def get_empirical_probability(logit, logits_train, y, T=1):\n",
    "    \n",
    "    valid = ((logits_train <= logit + T) * (logits_train >= logit - T));    \n",
    "    n_defaults = sum(valid[y==True]);\n",
    "    n_nodefaults = sum(valid[y==False]);\n",
    "    empirical_proba = n_defaults / (n_defaults + n_nodefaults);\n",
    "    return empirical_proba\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing set\n",
    "# we predict the probability of default for 100 values of balance\n",
    "\n",
    "n_test = 100\n",
    "X_test = np.zeros(shape=(n_test,2))\n",
    "X_test[0:n_test,0] = 1;\n",
    "X_test[0:n_test,1] = np.linspace(X[:,1].min(), X[:,1].max(), num=n_test)\n",
    "\n",
    "p_test = lab_2.predict(w, X_test)\n",
    "\n",
    "# we compute the logit values and their corresponding empirical probabilities of default\n",
    "logits_test = lab_2.logit(w, X_test)\n",
    "logits_train = lab_2.logit(w, X)\n",
    "p_empirical = [get_empirical_probability(logit, logits_train, y) for logit in logits_test ];\n",
    "\n",
    "# plot the graphs\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "ax.scatter(X[y==False,1], y[y==False], alpha= 0.2)\n",
    "ax.scatter(X[y==True,1], y[y==True], alpha= 0.2)\n",
    "ax.plot(X_test[:,1], p_test, color='black')\n",
    "ax.plot(X_test[:,1], p_empirical, ':', color='gray')\n",
    "\n",
    "ax.set_ylabel('Probability of default');\n",
    "ax.set_xlabel('Balance');\n",
    "ax.set_yticks([0, 0.25, 0.5, 0.75, 1.]);\n",
    "ax.legend(['logistic model for probability of default',\n",
    "           'empirically measured probability of default', \n",
    "           'No Default', 'Default'],  prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we still don't have a classifier. \n",
    "# All we need is to set a threshold on the predicted probabilities\n",
    "# Write a function 'predict_class' in the module to give\n",
    "# the predicted class for observations X and weights w.\n",
    "\n",
    "# Use that function here to assess the accuracy of the classifier\n",
    "# for different thresholds.\n",
    "\n",
    "# Accuracy = percentage correctly classified.\n",
    "def accuracy(w, X, y, threshold):\n",
    "    return np.mean(y == lab_2.predict_class(w, X, threshold))\n",
    "\n",
    "w = np.array([-10.63971053,   5.49188453])   # These are good weights!\n",
    "print(accuracy(w, X, y, 0.25))\n",
    "print(accuracy(w, X, y, 0.5))\n",
    "print(accuracy(w, X, y, 0.75))\n",
    "print(accuracy(w, X, y, 0.95))\n",
    "\n",
    "# Update your module function 'question_5' to report the accuracy for a threshold of 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
