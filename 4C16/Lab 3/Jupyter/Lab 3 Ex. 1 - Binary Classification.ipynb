{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PLEASE NOTE - THERE ARE TWO NOTEBOOKS IN LAB 3!\n",
    "\n",
    "# This notebook is for working through some classifier comparison techniques \n",
    "# it corresponds to Lecture Handouts 3 and 4.\n",
    "#\n",
    "# The questions are:\n",
    "#  -- Section 1: binary (two-class) classification (Lab 3 Ex. 1)  -- \n",
    "#   i) Evaluate & compare the ROC curves for different classifiers.\n",
    "#  ii) Influence the classifier performance by introducing class weights.\n",
    "#\n",
    "#  -- Section 2: multi-class classification (Lab 3 Ex. 2) --\n",
    "# iii) Evaluate & compare classifiers using the confusion matrix.\n",
    "#  iv) Classifier selection with practical criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Autoload setup (you don't need to edit this cell); instructions to: \n",
    "#   i) enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "#  ii) import the module 'lab_3' (which will contain your functions) in an autoreloadable way \n",
    "%aimport lab_3\n",
    "# iii) indicate that we want autoreloading to happen on every evaluation.\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// This cell disables scrollbars on our output, which is handier when we want to do lots of plots.\n",
    "// Don't edit this cell!\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#\n",
    "# Import useful libraries\n",
    "#\n",
    "# As well as numpy and matplotlib, this lab makes extensive use of a module \n",
    "# called 'scikit-learn' (sklearn); this provides a number of machine learning\n",
    "# tools and ready-built classifiers, as well as some datasets for experimentation.\n",
    "#\n",
    "# We are also using the module'pandas' for tabulated display of data.\n",
    "#\n",
    "#\n",
    "##############################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.7\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a utility function to show a scatter plot of binary-valued\n",
    "# data (i.e. 'y' has values 0 and 1), against attributes 'X'; X must\n",
    "# have at least two columns, and only the first two are used (as the \n",
    "# x and y axes for the scatter).\n",
    "\n",
    "def scatterplot_binary_data(X, y):\n",
    "    x_axis_min, x_axis_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_axis_min, y_axis_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.figure(figsize=plt.figaspect(1.0)).add_subplot(111)\n",
    "    # Plot the  points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
    "    ax.set_xlim(x_axis_min, x_axis_max)\n",
    "    ax.set_ylim(y_axis_min, y_axis_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes a dictionary of classifiers, and some training and test data,\n",
    "# and computes ROC data for each.\n",
    "#\n",
    "# The dictionary of classifiers should have entries 'name' -> scikit classifier.\n",
    "#\n",
    "# This does the fitting and the evaluation on each invocation.\n",
    "#\n",
    "# Not all the SciKit-Learn classifiers have a 'decision_function' method; for\n",
    "# those that don't, this method uses 'predict_proba' (for predicted probablity).\n",
    "# See the SciKit-Learn documentation for the distinction.\n",
    "\n",
    "def evaluate_classifiers_ROC(classifiers, X_train, y_train, X_test, y_test):\n",
    "    results = dict()\n",
    "\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        this_result = dict()\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        this_result['y_prediction'] = classifier.predict(X_test)\n",
    "        \n",
    "        if (hasattr(classifier, 'decision_function')):\n",
    "            this_result['y_score'] = classifier.decision_function(X_test)\n",
    "        else:\n",
    "            this_result['y_score'] = classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "        # fpr == False positive rate\n",
    "        # tpr == True positive rate\n",
    "        # auc == Area under curve\n",
    "        this_result['fpr'], this_result['tpr'], _ = skl.metrics.roc_curve(y_test, this_result['y_score'])\n",
    "        this_result['roc_auc'] = skl.metrics.auc(this_result['fpr'], this_result['tpr'])\n",
    "        results[classifier_name] = this_result\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function draws a set of ROC curves with labels.\n",
    "#\n",
    "# The input data, 'results', should be obtained by calling 'evaluate_classifiers_ROC'.\n",
    "#\n",
    "# The axis limits of the plot can be overridden, e.g. to zoom in on the upper-left corner.\n",
    "\n",
    "def plot_ROCs(results, xlim=[0.0, 1.0], ylim=[0.0, 1.05]):\n",
    "    ax = plt.figure(figsize=(8, 8)).add_subplot(111)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Comparison')\n",
    "\n",
    "    for classifier_name, result in results.items():\n",
    "        plt.plot(result['fpr'], result['tpr'],\n",
    "                 lw=2, label=classifier_name + ': (auc = %0.4f)' % result['roc_auc'])\n",
    "\n",
    "    plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function displays a table of classifier metrics.\n",
    "#\n",
    "# The input data, 'results', should be obtained by calling 'evaluate_classifiers_ROC'.\n",
    "\n",
    "\n",
    "def show_classifier_metrics(results, true_labels):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from IPython.display import display\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for name, result in results.items(): \n",
    "        m = {}  # This will store the metrics for the present classifier.\n",
    "\n",
    "        # 'FPR at total recall' is a measure of the ROC curve.\n",
    "        # It means: \"Set the decision threshold to the lowest value at which the classifier\n",
    "        # correctly identifies all the true positives in the test set.  At this threshold,\n",
    "        # what fraction of the negative examples in the test set were identified as positives?\".\n",
    "        # Lower values are better.\n",
    "        total_recall_index = np.argmax(result['tpr'] >= 0.999)\n",
    "        m['fpr_at_total_recall'] = result['fpr'][total_recall_index]\n",
    "        \n",
    "        # These metrics are not measures against the ROC curve, but rather measures\n",
    "        # of classification performance at a particular operating point.  For most\n",
    "        # classifiers this is 'classify as class having the highest score'.\n",
    "        predicted_labels = result['y_prediction']\n",
    "        m['precision'] = precision_score(true_labels, predicted_labels)\n",
    "        m['recall'] = recall_score(true_labels, predicted_labels)\n",
    "        m['f1'] = f1_score(true_labels, predicted_labels)\n",
    "        m['accuracy'] = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "        metrics[name] = m\n",
    "        \n",
    "    # We take the transpose so that each classifier is a column, rather than a row,\n",
    "    # and so the table grows vertically (convenient) rather than horizontally (inconvenient)\n",
    "    # as we add classifiers.\n",
    "    df = pd.DataFrame(metrics).transpose()\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The lab starts here!  Exercise 1 Part 1 -- synthetic data.\n",
    "#\n",
    "# Set up some off-the-shelf classifiers from the sklearn package\n",
    "\n",
    "classifiers = {\n",
    "    'Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Logistic':          LogisticRegression(),\n",
    "    'Linear SVM' :       SVC(kernel=\"linear\"),\n",
    "    'RBF SVM'    :       SVC(),\n",
    "    'Decision Tree' :    DecisionTreeClassifier(max_depth=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we generate and inspect some synthetic data, using one of the scikit-learn convenience\n",
    "# functions to create a fairly tricky source distribution.\n",
    "#\n",
    "# Experiment with different amounts of noise to see how it affects the data.\n",
    "\n",
    "X, y = skl.datasets.make_moons(n_samples=1000, noise=0.3, random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "scatterplot_binary_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (50/50), using another sklearn convenience function.\n",
    "X_train, X_test, y_train, y_test = skl.model_selection.train_test_split(X, y, test_size=.5, random_state=0)\n",
    "\n",
    "# Run this data through our classifiers, evaluating the ROC for each.\n",
    "results_synthetic = evaluate_classifiers_ROC(classifiers, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Show the result metrics and the ROC curves.\n",
    "xlim = [0.0, 1.0]\n",
    "ylim = [0.0, 1.05] \n",
    "plot_ROCs(results_synthetic, xlim, ylim)\n",
    "show_classifier_metrics(results_synthetic, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Quiz: Which classifier has the highest TPR for an FPR of 15%? (hint: change the axis limits!)\n",
    "#\n",
    "# Provide your answer in the function 'question_1' in the lab module ('lab_3.py')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This is Part 2: Real Data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up one of the standard datasets provided by the SciKit-Learn package.\n",
    "data = skl.datasets.load_breast_cancer()\n",
    "\n",
    "# In this data set, a label of '0' signifies malignant growth, and '1' signifies benign.\n",
    "print(\"Target names: \" + str(data.target_names))\n",
    "print(\"\\nDataset description:\\n\")\n",
    "print(data.DESCR)\n",
    "\n",
    "X_real = data.data\n",
    "# Scale each feature to be 0-mean with variance 1.  Strictly speaking this should\n",
    "# be done by fitting the transform to the training set and using that fit to\n",
    "# normalize both the training and test data (rather than incroporating the test\n",
    "# data into the computation of the fit).\n",
    "X_real = StandardScaler().fit_transform(X_real)\n",
    "\n",
    "# In this dataset, 'malignant' is coded with 0, but for ROC analysis it is\n",
    "# a little more convenient to think of detection, i.e. 'present' vs 'not present',\n",
    "# and so we will adjust the data so that 1 codes for 'malignant' (which we are \n",
    "# trying to detect).\n",
    "y_real = 1 - data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we split the data into training and test sets, 50/50.\n",
    "X_train, X_test, y_train, y_test = skl.model_selection.train_test_split(X_real, y_real, test_size=.5, random_state=0)\n",
    "\n",
    "# We push this data through the functions defined above to compute metrics and plot ROCs.\n",
    "results_real_data = evaluate_classifiers_ROC(classifiers, X_train, y_train, X_test, y_test)\n",
    "plot_ROCs(results_real_data)\n",
    "show_classifier_metrics(results_real_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There is a problem with the approach we've taken above: it considers the two\n",
    "# classes ('benign' & 'malignant') equally important (in the sense that the\n",
    "# base rate for classification corresponds to the relative prevalance of\n",
    "# each class in the training set).\n",
    "#\n",
    "# We want to be sure to detect malignancy, and so errors in classification of\n",
    "# (true) malignant samples should count for more than errors in classification\n",
    "# of benign samples, even if malignancy is less common than benignity.\n",
    "#\n",
    "# In the next cell we use class weights to influence our classifiers in this way.\n",
    "# We will only evaluate the Linear SVM classifier, as it got the best performance\n",
    "# in the 'fpr at 100% recall' rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up another set of classifiers, this time with class weights.\n",
    "\n",
    "classifiers_w = {\n",
    "    # The default is no class weights, which is the same as setting a weight of 1 on every class.\n",
    "    'SVM-no_weight': SVC(kernel=\"linear\", class_weight=None),\n",
    "    # 'balanced' is a special option which sets weights to offset any disparity in prevalence in the training data.\n",
    "    'SVM-balanced':  SVC(kernel=\"linear\", class_weight='balanced'),\n",
    "}\n",
    "\n",
    "# We will add in a set of classifiers with numerically set weights for class 1---malignant.\n",
    "# These weights are greater than 1, so they increase the importance of this class.\n",
    "weights = [1 + 0.05*x for x in range(1,11)]\n",
    "\n",
    "for w in weights:\n",
    "    classifier_name = ('SVM-weight-%0.2f' % w)\n",
    "    classifiers_w[classifier_name] = SVC(kernel=\"linear\", class_weight={1: w})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we evaluate this new set of classifiers, which use the weights, and examine the performance.\n",
    "results_real_data = evaluate_classifiers_ROC(classifiers_w, X_train, y_train, X_test, y_test)\n",
    "xlim = [0.0, 1.0]\n",
    "ylim = [0.0, 1.05]\n",
    "plot_ROCs(results_real_data, xlim, ylim)\n",
    "show_classifier_metrics(results_real_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Quiz: Identify the numerical value of the weight which minimizes the fpr for total recall\n",
    "#\n",
    "# Provide your answer in the function 'question_2' in the lab module ('lab_3.py')."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}