{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 - Text generation with LSTM\n",
    "#\n",
    "# Step 1 (not assessed): build and train a model to generate text in the style of a corpus.\n",
    "#\n",
    "# Based on the Keras text generation example (https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)\n",
    "#\n",
    "# Step 2: build a model to distinguish genuine from fake sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential modules\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Activation, Flatten, Dropout, Embedding, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import RMSprop, Adam, Nadam, SGD\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import save_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample an index from an array of predictions.\n",
    "#\n",
    "# The input array 'preds' should be the output of a text generation model.\n",
    "# The elements contain the values of the units in the final layer.\n",
    "# Each unit corresponds to a character in the text alphabet.\n",
    "# The final layer should have SoftMax activation, and thus the\n",
    "# value corresponds to the 'strength of prediction' of that character\n",
    "# as the next output value---so the maximum value indicates which character\n",
    "# is most strongly predicted (considerd most likely) as the next one.\n",
    "#\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Convert to high-precision datatype (we are going to be manipulating some\n",
    "    # very small values in this function)\n",
    "    preds = np.asarray(preds).astype('float64')  \n",
    "    \n",
    "    # The next line has the effect of raising each prediction value to the power 1/T.\n",
    "    # It's done using logs to improve numerical precision.  This is a kind of value-dependent\n",
    "    # scaling: for T < 1.0 (1/T > 1.0), small values are made smaller (proportionally) than \n",
    "    # large values (unlike a linear scaling, such as multiplication by 0.9, which scales all values\n",
    "    # the same).\n",
    "    #\n",
    "    # Example: Consider that we have only two symbols (letters) in our alphabet, and our \n",
    "    # probabilities are [0.2, 0.8].  A temperature of 1.0 means 'do not adjust the\n",
    "    # probabilities at all', so in this case there will be a 20% chance that the \n",
    "    # function will return 'symbol 0' and an 80% chance  that it will return 'symbol 1'.\n",
    "    # Note that symbol 1 is 4x more likely than symbol 0.\n",
    "    #\n",
    "    # Now: if we supply a temperature of 0.5, our probabilites will be raised to the\n",
    "    # power 1/0.5 = 2, becoming [0.04, 0.64].  These will then be normalized to sum to 1,\n",
    "    # but anyway it is clear that symbol 1 is here 16x (the square of 4x) more likely than \n",
    "    # symbol 0.\n",
    "    #\n",
    "    # Conversely, for a temperature of 2, our probabilities will be raised to 0.5 (square-rooted),\n",
    "    # becoming [.4472, 0.8944] - and so here symbol 1 is only 2x (sqrt of 4x) more likely than\n",
    "    # symbol 0.\n",
    "    #\n",
    "    # So: low temperatures make the distribution peakier, exaggerating the difference between\n",
    "    # values.  High temperatures flatten the distribution, reducing the difference between values.\n",
    "    #\n",
    "    # As the return value is a sample of the manipulated distribution, manipulating it to\n",
    "    # be peakier (by supplying a low temperature) makes the sample more conservative, i.e.\n",
    "    # more likely to pick the highest-probability symbol.\n",
    "    #\n",
    "    # Making the distribution flatter (by suppyling a high temperature) causes the\n",
    "    # sample to be less conservative, i.e. more likely to pick some lower-likelihood\n",
    "    # symbol.\n",
    "    #\n",
    "    # Phew!\n",
    "    preds = np.exp(np.log(preds) / temperature)\n",
    "    \n",
    "    preds = preds / np.sum(preds)  # ensure that probs sum to 1\n",
    "    probas = np.random.multinomial(1, preds, 1)  # take 1 sample from the distribution\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original corpus length: 600901\n",
      "length for training: 480139\n"
     ]
    }
   ],
   "source": [
    "# Decide how much data to use for training.\n",
    "# You might want to reduce this to ~100k for faster experimentation, and then bring it back\n",
    "# to 600k when you're happy with your network architecture.\n",
    "# IMPORTANT: mke sure you end up with a 57-symbol alphabet after reducing the corpus size!\n",
    "# If the number of symbols (shown in the next cell) gets smaller than it was with the full\n",
    "# corpus, bring your sample size back up.  This is necessary because the encoding used for\n",
    "# training must match that used for assessment.\n",
    "#desired_num_chars = 480*1000  # Max: 600893\n",
    "desired_num_chars = 480139  # Max: 600893\n",
    "\n",
    "random.seed(43)  # Fix random seed for repeatable results.\n",
    "\n",
    "# Slurp down all of Nietzsche from Amazon.\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('original corpus length:', len(text))\n",
    "\n",
    "start_index = random.randint(0, len(text) - desired_num_chars - 1)\n",
    "text = text[start_index:start_index + desired_num_chars]\n",
    "text\n",
    "print('length for training:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne is such a liar\n",
      "as the indignant man.\n",
      "\n",
      "27. it is difficult to be understood, especially when one thinks and\n",
      "lives gangasrotogati [footnote: like the river ganges: presto.] among\n",
      "those only who think and live otherwise--namely, kurmagati [footnote:\n",
      "like the tortoise: lento.], or at best \"froglike,\" mandeikagati\n",
      "[footnote: like the frog: staccato.] (i do everything to be \"difficultly\n",
      "understood\" myself!)--and one should be heartily grateful for the\n",
      "good will to some refinement of interpretation. as regards \"the good\n",
      "friends,\" however, who are always too easy-going, and think that as\n",
      "friends they have a right to ease, one does well at the very first to\n",
      "grant them a play-ground and romping-place for misunderstanding--one can\n",
      "thus laugh still; or get rid of them altogether, these good friends--and\n",
      "laugh then also!\n",
      "\n",
      "28. what is most difficult to render from one language into another\n",
      "is the tempo of its style, which has its basis in the character of the\n",
      "race, or to speak more physiologicall\n"
     ]
    }
   ],
   "source": [
    "# Let's have a quick look at a random exceprt.\n",
    "#\n",
    "# Caution: Nietzsche might drive you mad: dare you behold more than 1000 of his terrible chars..? \n",
    "sample_length = 1000\n",
    "\n",
    "random.seed(None)  # Seeds random from current time (so re-eval this cell for a new sample).\n",
    "\n",
    "start_index = random.randint(0, len(text) - sample_length - 1)\n",
    "print(text[start_index:start_index+sample_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¤', '¦', '«', 'ã']\n"
     ]
    }
   ],
   "source": [
    "# Establish the alphabet (set of symbols) we are going to use.\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "print(chars)\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))  # Map to look up index of a particular char (e.g. x['a'] = 0)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))  # Map to look up char at an index (e.g. x[0] = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 160033\n"
     ]
    }
   ],
   "source": [
    "# Establish a training set of semi-redundant (i.e. overlapping) sequences of maxlen characters.\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []  # Not syntactic sentences, but just sequences of 40 chars pulled from the corpus.\n",
    "next_chars = [] # next_chars[n] stores the character which followed sentences[n]\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160033, 40, 57)\n",
      "(160033, 57)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to one-hot encoding.\n",
    "# 'x' will contain the one-hot encoding of the training 'sentences'.\n",
    "# 'y' will contain the one-hot encoding of the 'next char' for each sentence.\n",
    "#\n",
    "# \n",
    "# Let's consider that we have N sentences of length L:\n",
    "#\n",
    "# The 'native' encoding is an NxL matrix where element [n][l]\n",
    "# is the symbol index for character at index (l) of sentence (n)\n",
    "# (e.g., say, 5, corresponding to 'e').\n",
    "#\n",
    "# The one-hot encoding is an NxLxS matrix, where S is the \n",
    "# number of symbols in the alphabet, such that element [n][l][s]\n",
    "# is 1 if the character at index (l) in sentence (n) has the\n",
    "# symbol index (s), and 0 otherwise.\n",
    "def onehot_encode(sentence, maxlen):\n",
    "    x = np.zeros((maxlen, len(chars)), dtype=np.bool)\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[t, char_indices[char]] = 1\n",
    "    return x\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x[i,:,:] = onehot_encode(sentence, maxlen)\n",
    "    y[i, :] = onehot_encode(next_chars[i], 1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the generator model: a single GRU layer with 128 cells.\n",
    "generator_model = Sequential()\n",
    "generator_model.add(GRU(128, input_shape=(maxlen, len(chars))))\n",
    "#generator_model.add(LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "generator_model.add(Dense(len(chars)))\n",
    "generator_model.add(Activation('softmax'))\n",
    "\n",
    "# You could experiment with NAdam instead of RMSProp.\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "generator_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "trained_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_list(seed_list, length=400, temperature=0.25):\n",
    "    sentence_list = [];\n",
    "    generated_list = [];\n",
    "    n = len(seed_list)\n",
    "    # copy lists\n",
    "    for seed in seed_list:\n",
    "        sentence_list.append(seed[:])\n",
    "        generated_list.append(seed[:])    \n",
    "    \n",
    "    for i in range(length):\n",
    "      \n",
    "        workdone = (i+1)*1.0 / length\n",
    "        sys.stdout.write(\"\\rgenerating sentences: [{0:20s}] {1:.1f}%\".format('#' * int(workdone * 20), workdone*100))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        x_pred_list = np.zeros((n, maxlen, len(chars)))\n",
    "        for j, sentence in enumerate(sentence_list):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred_list[j, t, char_indices[char]] = 1.\n",
    "\n",
    "        start = time.time()\n",
    "        pred_list = generator_model.predict(x_pred_list, verbose=0)\n",
    "        end = time.time()\n",
    "\n",
    "        for j in range(n):\n",
    "            next_index = sample(pred_list[j,:], temperature)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated_list[j] += next_char\n",
    "            sentence_list[j] = sentence_list[j][1:] + next_char\n",
    "    \n",
    "    sys.stdout.write(' - done\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return generated_list\n",
    "\n",
    "def print_sentences(seeds, sentences):\n",
    "    for seed, sentence in zip(seeds, sentences):\n",
    "        print('-'*5)\n",
    "        sys.stdout.write('\\x1b[32m')\n",
    "        sys.stdout.write(sentence[0:len(seed)])\n",
    "        sys.stdout.write('\\x1b[34m')\n",
    "        sys.stdout.write(sentence[len(seed):-1])\n",
    "        sys.stdout.write('\\x1b[m')\n",
    "        sys.stdout.write('\\n')    \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def pick_sentences(n, maxlen):\n",
    "    global text    \n",
    "    start_index_list = np.random.randint(len(text) - maxlen - 1, size=(1, n)).flatten().tolist()\n",
    "    seed_list = [] \n",
    "    for start_index in start_index_list:\n",
    "        seed_list.append(text[start_index: start_index + maxlen])\n",
    "    return seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 18s 110us/step - loss: 2.2970\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 14s 88us/step - loss: 1.8250\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.6725\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 15s 93us/step - loss: 1.5936 \n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m, from the are in the strength of the for the stranges the of the schooly there is a persigness the strungly the problems of the strength the individual the for the indiscives striggle for the has the the significal the strength the strength of the mast in the strength of the whole in striggle the call of the schoother the sare and sensity of the schoother the has the his problems of the his pass\u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34min the indesirn the schooly the find a mast significal the school of the schooly of the sense there is strength in the schooly the schooly his plansions, they he has the has the strength of the say the schole the his perhaps the soul and in the self-consigness of the his passive the significations of the strength in the shall of the whole he has the schooly and instincts there is a conscience the\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mht his his conscience of the whole the strength of the schollow the such a sighing of the striggle the species the has the his his only the self-general striggle the says the has the whole there is a surfing the his his striggle of the strength of the striggle of the for the the strungly for the striggle of the has the has the sart, a striggle the general strength of the saint of the sense, in th\u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.5407\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 14s 90us/step - loss: 1.5014\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 15s 91us/step - loss: 1.4739\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.4536\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m of simplication and all the such and and self-consequently and satisfye for all the still and different and and self-cased in a consequences, and in it is all the say the simply and self-case and an in the such and and supposing and the supposing and the same with the thinking and and all the such and all the and the sense and self-contrority and all the such and and self-called and finds the se\u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34mlitter will life and\n",
      "self-signom and the sense and self-same to a most and self-stand and and self-stand and all the such an in the some and self-cased the such an in the same and different and\n",
      "the sense and and self-same in the such and and all the different and and and all the finally and all the self-sense and and this self-call of all the self-same to a perplese and self-controm and self-case\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mht for man in the sense and all difficult the satisfye and different and self-consequences. for the self-chanically and self-case and as it is a problem will bitter and says and self-called for the for the in this sense and and in a self-contrority and self-changered the sense and all the explanations and in a self-finally and and different for the finds and self-consequences. the sense and every\u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.4355\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 15s 92us/step - loss: 1.4209\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 14s 88us/step - loss: 1.4091\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 15s 96us/step - loss: 1.3986\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m and fan and the consequently and and consequence of a concentle of the feelings of the fact the most distance, and to be against and and the condition of the most and and and consequence of the end the most a good and and be not a german many fantary\n",
      "with a consider and the faith and the fact the most a good and and and certain construmity and the greates to a consequence of the experiences and \u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34mand all and part for a condition of the fact the consequence of all instinct of every soursed and and consequence and and consequence of a reason of the most distance, and and the consequence and instinctive and the conditions and the finds the fact the feelings and and more destion of the end of a promed and who decentions and the real some and the condition of the formire far as a german who di\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mhbour and consequences of the feelings of a consider and more requires to be agains and to be agains and to instinct of the conditions and and the german and fact the frence of the feelings. the fact the part of all and of the most accordions and fantation of the feeling, and and and and condition of the feeling for instance, and and consequence of the fact the most and far a differently words an\u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 90us/step - loss: 1.3910\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 14s 90us/step - loss: 1.3827\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 15s 94us/step - loss: 1.3780\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 15s 92us/step - loss: 1.3711\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m is that will a still that which will a soul, which we all struggle that with the world and struggle and with the pressions of the contratical of the controus that wisticism and the nature that which we a not be the fact, the feel its requite it with a characterity of the content that will a structure and a charable that with the contraint of the fact that with the world and cannot happiness of t\u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34mthe same that with the fact that will the contrating a presticlate and the castacitity and class with the matter, that which will a structude that with the presered and the fact that will the caste with the fact that wishes to say, and that which has it will all that is a new the controut of the exception of the fact is a still the presere a charable and struggle with the fact that with the fact \u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mhbonest and with the contrait frequently the pressions and that with the world and decide to the fact that will to presting that we all the fictly the sake of the persies and the presticition that which will a structure and the controut of the feelings of the presered as a charable that which we all the fact that will that wist of the pressions and struggle with the world and contrary with the co\u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 15s 94us/step - loss: 1.3652\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 15s 91us/step - loss: 1.3602\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 15s 91us/step - loss: 1.3556\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.3500\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34ms, the the explaned than the here are he is a world, for the who are have to be understand the uncless, the believe in the unclusing of the and a the an any have how to be uncenture itself to the the of the and a self-deceived of the the free sphere and an any one itself to the and the than the has been and a really the because have the and the anciently the an and a man of the his precentively t\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34mthe and an and has been an any more hand of the inconsisted and a person should be the an and has been an art been the the the final uncenture of the of the and a consequences, the the historical sense of the inclue the the incensively and more the been the uncenture and a strenuth is a sense of the inclue the the uncless, the the the and the precisely the for the and the beart and a philosophy o\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mht of the an intent of the the explaned the believe in the here are the and an any objection, the the present to hear the beart of the for the instincts, the the sense of the and an any handsence of the ancient and the delick of the beart and not be understood: the the and the heart he the any hands of the ance the the have to be uncenture and man to be the uncenture and all the hands of the the \u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 13s 83us/step - loss: 1.3478\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 14s 86us/step - loss: 1.3425\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 14s 88us/step - loss: 1.3419 0s - los\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 88us/step - loss: 1.3379\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m of the soulf its probably that is that is that is not the probably from reason that is almost expression what is productuse of the same that is not the confition of the spirit, who who delicate and confiding that who has been the probably that is stronger and who can be still and confiding that is that is that is that is all that that is not the probably such a probably that is confition of the \u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34malong the conflittic spready of the conflittic to the probably who has been and responsition of the same that it would he who delicate as its practical form of the spirit,\" is been all the spirit, in the confition of the spirit, who can be such a problem of all that is to say the conflitting the opposition of the confition of the most his bad that is almost the confition of the condition of the s\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mhbous of the probably that who are strongest one of the confition of the probably that is to say of the conflittic to the subseration, and in spire, the conflittic spirits, that is that is to say that is not the confliction, and confined, the probably and more sentiments and more soul, that is always without for the conflittic spirits when the confition of the more profound destrumous to the conf\u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.3336\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.3321\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.3277\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 88us/step - loss: 1.3253\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m of mankind, or moralism manking, and something contemplates of mankind, and seems the most matter, and such as a matter of mankind of men of moralists, and subject matter and metaphysically and more moralists, and more mankind, and sufficiently men of mankind, which was moralists, and such as a more impertable men, in a moralists, and mankan eaces as intoxicated matter of mankind of mankind of m\u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34malso matter and seems the sake of all mankind and contempon one should despises is as much as and moralism, as an and severe, as a matter of mankind, the most matter of men of mankind, and sufferent the most mankan enders of moralists of mankind, and matter of mankind, of the most made and most manking of men, with one should kentively and seems the hand of such as a matter, and seems to say, and\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mhborms of men of moralists and such as a moralists, and mankind of the most manking of mankind, and seems the most moralists, and superficiality.--in the most manking of the most metaphysically and something of mankind, and subject matter and such as a moralists, and moralism manking of mankind, and mort and moralists and self-trying of mankind of mankind, and mankind and such a moralists matter \u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 90us/step - loss: 1.3220 0s - los\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 15s 92us/step - loss: 1.3203 1s\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 14s 90us/step - loss: 1.3179\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.3168\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34ms, and which which has the profounder and distapt and spiritual fundamentaly and all south, and all strict of the same spiritual feelings, and all the philosophy and all science, and also which which which which we should be the philosophy is a stricteschery and sentiments and being and all south in all the profounerations of the same spiritually and according to the destrunt the concepting that \u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34malso the profounder and all strict of the fact that is also against our new fruing all the profounes, and which which which we which that is also all the spiritual frenchtatical against a strictesch and all the profounder and spiritual form of the fact to the profounders and the possible for all strict of all the profounders and profounality, and in all the profounerates and spiritual freeds a pr\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mhbour and cally the spiritual for science, and has been stricture and strictus for the fact that is also a gradually and all science, and hears in all sount are also of the fact of the spirity, and has not his spiritual frenchtatics, in the spiritual form of the spiritual fundamentally and the profounerates and strictus seek the profounders and morality, as a strictest conceptions and profounders\u001b[m\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 89us/step - loss: 1.3142\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 15s 93us/step - loss: 1.3138\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 14s 87us/step - loss: 1.3126\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 86us/step - loss: 1.3088\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m of the something that is the that the indepented the point of the continguate of the something that the indepthing the same that the philosopher and the same the than the fooloness, that the former to the the philosopher of the fooloness, that the philosopher of the for the \"the \"most taken the than the philosopher and that the consistict, the the point of the work of the for the philosopher tha\u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34mat the philosopher that the end of the for the degined the for the point of the long, that the philosopher that the consideration, the proposion of the work and that the \"the \"the \"the \"the longer that the the longer do they are the consideration of the for the point of the \"the \"the \"true of the long) to the contingure of the consideration, the for the contingure of the fooloness, that the for t\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mhtoted that the contrary for the contingure of the for the point of the same the for the the contrary, that has the one of the spiritual for the for the point of the same the something love to the has the for the \"not of the formulas of the same the compliise of the \"the \"for the \"the point of the contingure of the same that the the the most the profounders of the former to the whole and and the \u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 14s 86us/step - loss: 1.3070\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 14s 85us/step - loss: 1.3049\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 13s 84us/step - loss: 1.3024\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 14s 85us/step - loss: 1.3027\n",
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mther aside. for it is, to all appearance\u001b[34m of such a state of the soul and something constantation, which has been such a state the germans of the saint, and the serve of the most maints, the senses, with which we knowledge of the senses, this construits of the senses of the said to be the signify of the senses of the strives of this conscience, which we know that a states and the spirits of the senses, which still of the senses, that th\u001b[m\n",
      "-----\n",
      "\u001b[32miment que la religion est un produit de \u001b[34mthe saint, that is a soul that his presence of this construits of the spirits of think will be sourts of the saint of this conscience, which was the conscience, and with which still still to be the conscience of think will be the present think of the said to say has been and compressed that the regard of the soul of this questions of the senses, which well that still to be still of the senses, th\u001b[m\n",
      "-----\n",
      "\u001b[32mf surrender, of sacrifice for\n",
      "one's neig\u001b[34mht that in the strives of think will be sourt of the senses, which we knowledge of this senses of the soul and straiged in problems of the contrarictors of the saint,\" and that is that it is think will be the senses of this regard of the spirits of the soul, without believe of this conscience of this construits which still still still to a conscience, and such a state the recontinudents and think\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# Generate 3 seeds which we will use to inspect the progress of our training:\n",
    "preview_seeds = pick_sentences(3, maxlen=40)\n",
    "\n",
    "# Train the model, output generated text after each iteration\n",
    "for iteration in range(1, 11):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    generator_model.fit(x, y,\n",
    "                  batch_size=1024,\n",
    "                  epochs=4)\n",
    "\n",
    "    generated_sentences = generate_sentence_list(preview_seeds)\n",
    "    print_sentences(preview_seeds, generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32me psychologist\n",
      "who has discovered this r\u001b[34mecontrared and strong than of this cons\u001b[m\n",
      "-----\n",
      "\u001b[32ms that to knowledge the highest utility \u001b[34mof the strives of the strives of the so\u001b[m\n",
      "-----\n",
      "\u001b[32m easier to go over to a\n",
      "really emancipat\u001b[34med and think well that in the saint, an\u001b[m\n",
      "-----\n",
      "\u001b[32mfering, to sit still, to exercise patien\u001b[34mce, which well that the religious serve\u001b[m\n",
      "-----\n",
      "\u001b[32mned jargon is: psychological\n",
      "observation\u001b[34m of the soul, with which well that the \u001b[m\n",
      "-----\n",
      "\u001b[32meen the\n",
      "specialties of science and philo\u001b[34msophers of the senses, which well with \u001b[m\n",
      "-----\n",
      "\u001b[32m the climax, the attained climax of\n",
      "mank\u001b[34mind will be sourt of the saint, that is\u001b[m\n",
      "-----\n",
      "\u001b[32m christian period of european\n",
      "history, a\u001b[34mnd the senses, which well that it is so\u001b[m\n",
      "-----\n",
      "\u001b[32mower, a strange, still unconquered\n",
      "enemy\u001b[34m with the strive will to be of the sain\u001b[m\n",
      "-----\n",
      "\u001b[32mt to speak of syllables) of a page--he r\u001b[34mesked and self-resporting spirits of th\u001b[m\n",
      "-----\n",
      "\u001b[32msea and halcyon self-sufficiency, the go\u001b[34mod will to be the morality, the servelt\u001b[m\n",
      "-----\n",
      "\u001b[32m: vanity is\n",
      "an atavism.\n",
      "\n",
      "262. a species \u001b[34mof the saint, and the saint, and the se\u001b[m\n",
      "-----\n",
      "\u001b[32mishment. they feel themselves already fu\u001b[34mrther of the senses, which well with su\u001b[m\n",
      "-----\n",
      "\u001b[32mll\n",
      "and desire for power, threw themselve\u001b[34ms in the saint, that is strong than of \u001b[m\n",
      "-----\n",
      "\u001b[32msiastic about the good, true, and beauti\u001b[34mful recents of the saint, that the read\u001b[m\n",
      "-----\n",
      "\u001b[32ml with evil and actually brings requital\u001b[34m to the strives of the said, that it wi\u001b[m\n",
      "-----\n",
      "\u001b[32mn, that is,\n",
      "as error) but which is merel\u001b[34my some order of the senses of the sense\u001b[m\n",
      "-----\n",
      "\u001b[32mmaster, who, on account of his lighter, \u001b[34mand will be some hand we knowledge of t\u001b[m\n",
      "-----\n",
      "\u001b[32mtop to the exaggeration with\n",
      "which the u\u001b[34mnconsciously become soul and construits\u001b[m\n",
      "-----\n",
      "\u001b[32mmost remarkable creation of richard wagn\u001b[34mer and self-resporting strives and self\u001b[m\n",
      "-----\n",
      "\u001b[32mliminated, and vestiges of allegorical a\u001b[34mctions of the senses of the senses, whi\u001b[m\n",
      "-----\n",
      "\u001b[32mions; as others, too volatile and excita\u001b[34mments of the said, we will be sourt, an\u001b[m\n",
      "-----\n",
      "\u001b[32m the hidden and forgotten\n",
      "treasure, the \u001b[34mserve of the senses, which well with su\u001b[m\n",
      "-----\n",
      "\u001b[32mes do not deceive. the fact thereby beco\u001b[34mme with the strives of the strives of t\u001b[m\n",
      "-----\n",
      "\u001b[32mlow. this evinces\n",
      "much simplicity--as if\u001b[34m this senses of strivings, which well w\u001b[m\n",
      "-----\n",
      "\u001b[32mber of frequently recurring experiences\n",
      "\u001b[34mof the saint, and this strives of the s\u001b[m\n",
      "-----\n",
      "\u001b[32mm experience how a dreamer will transfor\u001b[34mm with the strives of the saint, that t\u001b[m\n",
      "-----\n",
      "\u001b[32m are nevertheless the\n",
      "worthy grandchildr\u001b[34me with still recognized that the senses\u001b[m\n",
      "-----\n",
      "\u001b[32mf ascribing the highest utility to itsel\u001b[34mf and strong than of the senses, which \u001b[m\n",
      "-----\n",
      "\u001b[32mnd more common\n",
      "forms of these living cry\u001b[34m that in the saint, that is to be some \u001b[m\n",
      "-----\n",
      "\u001b[32m jews with the least hesitation, for ins\u001b[34mtance, which we know the strives of thi\u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "every hand that lays hold of him shrink\u001b[34m of the straiged of the more still stil\u001b[m\n",
      "-----\n",
      "\u001b[32mor it rests upon the erroneous\n",
      "assumptio\u001b[34mn of the senses, which well that it is \u001b[m\n",
      "-----\n",
      "\u001b[32mas a degenerating form of political orga\u001b[34mnssitude and construments, which well w\u001b[m\n",
      "-----\n",
      "\u001b[32m welfare) wants to have\n",
      "any knowledge or\u001b[34m not that the respect of the senses, wh\u001b[m\n",
      "-----\n",
      "\u001b[32mein is comprised the\n",
      "tremendous mission \u001b[34mof the same think will be the saint, an\u001b[m\n",
      "-----\n",
      "\u001b[32m questions like, \"why have sympathetic a\u001b[34mppearances of the said to be manness of\u001b[m\n",
      "-----\n",
      "\u001b[32ms the awkward\n",
      "craftiness which first get\u001b[34mthess with which well--only serve every\u001b[m\n",
      "-----\n",
      "\u001b[32mics. later, indeed, he acquires\n",
      "distrust\u001b[34m of the soul and self-resposs in the se\u001b[m\n",
      "-----\n",
      "\u001b[32mr they appear to\n",
      "the living in dreams.\" \u001b[34mwith the still still so manity of the s\u001b[m\n",
      "-----\n",
      "\u001b[32munder thick dark\n",
      "ice, and is a divining-\u001b[34m-that is something will be sourt of the\u001b[m\n",
      "-----\n",
      "\u001b[32mt is\n",
      "expediency, expediency, expediency,\u001b[34m that is also will be the strives of th\u001b[m\n",
      "-----\n",
      "\u001b[32mhich precludes all danger that\n",
      "the spiri\u001b[34mts of the soul and self-resposs in the \u001b[m\n",
      "-----\n",
      "\u001b[32ms\n",
      "of the intellect, to wit: \"whence did \u001b[34mthe conscience of strives of the soul a\u001b[m\n",
      "-----\n",
      "\u001b[32mminess, and solitariness, is assuredly n\u001b[34mot the soul and self-resposition of the\u001b[m\n",
      "-----\n",
      "\u001b[32mle happiness; such a person, who believe\u001b[34m of the saint, and the senses, and stil\u001b[m\n",
      "-----\n",
      "\u001b[32mscetics and\n",
      "\"anti-natural\" fanatics. fin\u001b[34mally the senses, which well that the st\u001b[m\n",
      "-----\n",
      "\u001b[32me mastersinger: it is a piece of magnifi\u001b[34mces of the senses of the senses, the se\u001b[m\n",
      "-----\n",
      "\u001b[32my craftiness, with\n",
      "which the problem of \u001b[34mstrivings, that the reputsed to be the \u001b[m\n",
      "-----\n",
      "\u001b[32mfrom being philosophers themselves! even\u001b[34m still still still still still of the s\u001b[m\n",
      "-----\n",
      "\u001b[32msiegfried,\n",
      "that very free man, who is pr\u001b[34mecisely that is also will be the strive\u001b[m\n",
      "-----\n",
      "\u001b[32mefore, only a synthesis\n",
      "which has been m\u001b[34manifold of the saint, and think will be\u001b[m\n",
      "-----\n",
      "\u001b[32mm, it is manifest that the world is neit\u001b[34mher that is also will be the saint, and\u001b[m\n",
      "-----\n",
      "\u001b[32mn one point the very knack and lucky gra\u001b[34mnded that the respect to the strive per\u001b[m\n",
      "-----\n",
      "\u001b[32me of\n",
      "managing grosser affairs, and for s\u001b[34mtill still still so manity of the sense\u001b[m\n",
      "-----\n",
      "\u001b[32mient\n",
      "greeks is the irrestrainable stream\u001b[34ments, which we know how that the ready \u001b[m\n",
      "-----\n",
      "\u001b[32mis devilry and concealed\n",
      "insatiability, \u001b[34mwhich we know hold of the saint, and th\u001b[m\n",
      "-----\n",
      "\u001b[32me way\n",
      "for the coming of the philosopher;\u001b[34m all of the senses, which we know the s\u001b[m\n",
      "-----\n",
      "\u001b[32me maturity in every\n",
      "culture and art, the\u001b[34m senses, which well with such a state t\u001b[m\n",
      "-----\n",
      "\u001b[32mginates, and a type becomes established \u001b[34mto the reasons of the senses, that is a\u001b[m\n",
      "-----\n",
      "\u001b[32m approved of; he passes the judgment: \"w\u001b[34mhat seems to be some compressed to the \u001b[m\n",
      "-----\n",
      "\u001b[32m is good and evil. it must then sound ha\u001b[34ms been the senses, which well with such\u001b[m\n",
      "-----\n",
      "\u001b[32mrhaps necessary for greatness of soul, t\u001b[34mhat wishes to the reasons of the senses\u001b[m\n",
      "-----\n",
      "\u001b[32m grappling hook of\n",
      "knowledge; to that ma\u001b[34mn with the soul, with which that the re\u001b[m\n",
      "-----\n",
      "\u001b[32ments of worth,\n",
      "and distinctions of worth\u001b[34m that is also will be the strive perhap\u001b[m\n",
      "-----\n",
      "\u001b[32mception causa sui is something\n",
      "fundament\u001b[34ms of the spirits of the strives of the \u001b[m\n",
      "-----\n",
      "\u001b[32ms the very earliest inward prompting of \u001b[34mthe soul, which we knowledge of the sen\u001b[m\n",
      "-----\n",
      "\u001b[32mms among\n",
      "savage as well as among civiliz\u001b[34mation of the senses, which well with su\u001b[m\n",
      "-----\n",
      "\u001b[32mtructive, as well as creative and fashio\u001b[34mn with which well that the ready still \u001b[m\n",
      "-----\n",
      "\u001b[32mo begin with, that the term \"philosopher\u001b[34m of the senses, with which we knowledge\u001b[m\n",
      "-----\n",
      "\u001b[32mnd\n",
      "feelings one can penetrate deep into \u001b[34mthe said, which well with such a servel\u001b[m\n",
      "-----\n",
      "\u001b[32ms of consoling, lightening,\n",
      "charming exi\u001b[34msted to the ready strives of the strive\u001b[m\n",
      "-----\n",
      "\u001b[32mrds seem almost unbreakable? in\n",
      "the case\u001b[34ms and self-resposs in the senses, that \u001b[m\n",
      "-----\n",
      "\u001b[32m virtues,\n",
      "although naturally they are no\u001b[34m longer and self-respositions of the so\u001b[m\n",
      "-----\n",
      "\u001b[32m sensible, even in our\n",
      "waking moments, o\u001b[34mr this man will be something will be th\u001b[m\n",
      "-----\n",
      "\u001b[32mcognized their own strength and\n",
      "love of \u001b[34mthe said of the saint, the strives of t\u001b[m\n",
      "-----\n",
      "\u001b[32m tried to bring laughter into bad repute\u001b[34m; and that the senses, which well that \u001b[m\n",
      "-----\n",
      "\u001b[32med! it is so nice and such a\n",
      "distinction\u001b[34m of the senses, which well with such a \u001b[m\n",
      "-----\n",
      "\u001b[32m_ which we form (nature = world, as noti\u001b[34mve spirits of the strives of the said t\u001b[m\n",
      "-----\n",
      "\u001b[32mions) to be the cause of the decline of\n",
      "\u001b[34mthis senses of the said to be the great\u001b[m\n",
      "-----\n",
      "\u001b[32ma\n",
      "future!\n",
      "\n",
      "224. the historical sense (or\u001b[34m accompanits of the strives of the sain\u001b[m\n",
      "-----\n",
      "\u001b[32min respect to the\n",
      "valuation of things, i\u001b[34mn the senses, which well with such a st\u001b[m\n",
      "-----\n",
      "\u001b[32me eyes\n",
      "are open and the senses awake, is\u001b[34m all think of the still still still so \u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "\u001b[32m whether he who experiences\n",
      "them is not \u001b[34mthat the senses, we knowledge of the se\u001b[m\n",
      "-----\n",
      "\u001b[32mg of another.\n",
      "\n",
      "242. whether we call it \"\u001b[34mgod and strives of the saint, that is a\u001b[m\n",
      "-----\n",
      "\u001b[32mkind\n",
      "consists simply in the fact that th\u001b[34me signify of this construits of the sai\u001b[m\n",
      "-----\n",
      "\u001b[32msinterestedly.\" there have been philosop\u001b[34mher with which we knowledge of the sain\u001b[m\n",
      "-----\n",
      "\u001b[32mstic affinities is very hard to find; st\u001b[34mill still still still be soul, which we\u001b[m\n",
      "-----\n",
      "\u001b[32ma garden--or as music on the waters at e\u001b[34mvery one should be still still still of\u001b[m\n",
      "-----\n",
      "\u001b[32mams\n",
      "carry us back to the earlier stages \u001b[34mof the soul, with which we knowledge of\u001b[m\n",
      "-----\n",
      "\u001b[32mnuation and spiritualization by the symb\u001b[34modism that it will be the senses of the\u001b[m\n",
      "-----\n",
      "\u001b[32mt of historical justice in\n",
      "a determined \u001b[34mto the senses of the same strivings, th\u001b[m\n",
      "-----\n",
      "\u001b[32m again, from\n",
      "time to time, that is, his \u001b[34mprocess of the strives of the strives o\u001b[m\n",
      "-----\n",
      "\u001b[32mcertain:\n",
      "imagine to yourselves indiffere\u001b[34mnts and self-contritually constraint, a\u001b[m\n",
      "-----\n",
      "\u001b[32mt sufficient to use the same words in or\u001b[34mder to the ready still still still stil\u001b[m\n",
      "-----\n",
      "\u001b[32mal\n",
      "under passionate grimaces what he kne\u001b[34mws because of the saint, that is also w\u001b[m\n",
      "-----\n",
      "\u001b[32ms also still--youth!\n",
      "\n",
      "32. throughout the\u001b[34m senses, which well with such a state o\u001b[m\n",
      "-----\n",
      "\u001b[32mcy). it is thus, in effect, that method \u001b[34mof the saint, and the senses, that is s\u001b[m\n",
      "-----\n",
      "\u001b[32mion. and in fact, the training of the in\u001b[34mtercongeness, with which well with such\u001b[m\n",
      "-----\n",
      "\u001b[32mvided one has\n",
      "understood in its full pro\u001b[34mparted and think will be states of thin\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# For a more complete inspection, print out a load of sentences:\n",
    "#\n",
    "num_sentences = 100             # how many to generate\n",
    "sentence_length = 40            # 100--400 is good\n",
    "#sample_temperature = 0.25       # see discussion of temperature up near the top\n",
    "sample_temperature = 0.15       # see discussion of temperature up near the top\n",
    "\n",
    "start_index_list = np.random.randint(len(text) - maxlen - 1, size=(1, num_sentences)).flatten().tolist()\n",
    "preview_seeds = [] \n",
    "for start_index in start_index_list:\n",
    "    preview_seeds.append(text[start_index: start_index + maxlen])\n",
    "\n",
    "generated_sentences = generate_sentence_list(preview_seeds, length=sentence_length, temperature=sample_temperature); \n",
    "print_sentences(preview_seeds, generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a checkpoint, which will let you download and re-upload (or add to git) this model.\n",
    "save_model(generator_model, './generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating sentences: [####################] 100.0% - done\n"
     ]
    }
   ],
   "source": [
    "# Generating the training fake sentences for the Discriminator network\n",
    "#\n",
    "# These are saved to the file 'fake.pkl' -- you could download this to your\n",
    "# user drive and re-upload it in a subsequent session, to save regenerating\n",
    "# it again (in which case you don't need to evaluate this cell).\n",
    "\n",
    "training_seeds = pick_sentences(3000, maxlen=40)\n",
    "training_generated_sentences = generate_sentence_list(training_seeds, length=40)\n",
    "# Strip out the initial 40 chars (the seed sequence, which is genuine data from the corpus).\n",
    "for i, sentence in enumerate(training_generated_sentences):\n",
    "    training_generated_sentences[i] = sentence[40:40+40]\n",
    "    \n",
    "output = open('fake.pkl', 'wb')\n",
    "pickle.dump(training_seeds, output)\n",
    "pickle.dump(training_generated_sentences, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set from the file\n",
    "pkl_file = open('fake.pkl', 'rb')\n",
    "training_seeds = pickle.load(pkl_file)\n",
    "training_generated_sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 50:50 set of 'fake' (generated) and genuine sentences:\n",
    "num_generated = len(training_generated_sentences)\n",
    "training_real_sentences = pick_sentences(num_generated, maxlen=40)\n",
    "\n",
    "all_training_sentences = training_generated_sentences + training_real_sentences\n",
    "n = len(all_training_sentences)\n",
    "x = np.zeros((n, 40, len(chars)))\n",
    "y = np.zeros((n, 1))\n",
    "\n",
    "for i, sentence in enumerate(all_training_sentences):\n",
    "    x[i, :, :] = onehot_encode(sentence, maxlen=40)\n",
    "y[num_generated:] = 1  # Encodes the fact that sentences with indexes larger than (num_generated) are real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "(6000, 40, 57)\n",
      "compiled.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, 1024)              3323904   \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 3,324,929\n",
      "Trainable params: 3,324,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "# Define some layers here..\n",
    "print(x.shape)\n",
    "#print(x.shape)\n",
    "#print(y.shape)\n",
    "#print(n)\n",
    "\n",
    "# Use your layers to create the model.\n",
    "\n",
    "#inputs = Input(shape=(40, 59))\n",
    "#h = LSTM(256)(inputs)\n",
    "#h = Dropout(0.2)(h)\n",
    "#h = Dense(1024, activation='relu')(h)\n",
    "#h = LSTM(256, return_sequences=False)(h)\n",
    "#h = Dense(512, activation='tanh')(h)\n",
    "\n",
    "#output = Dense(1, activation='softmax')(h)\n",
    "\n",
    "discriminator_model = Sequential()\n",
    "#discriminator_model.add(Embedding(4020, 40, input_length=59))\n",
    "discriminator_model.add(GRU(1024, dropout = 0.2, return_sequences=False, recurrent_dropout = 0.2, input_shape=(maxlen, len(chars))))\n",
    "#discriminator_model.add(LSTM(256, recurrent_dropout=0.0, return_sequences=False, input_shape=(maxlen, len(chars))))\n",
    "#discriminator_model.add(LSTM(128, return_sequences=True))\n",
    "#discriminator_model.add(Conv1D(64, 5, activation='relu', padding='valid', input_shape=(maxlen, len(chars))))\n",
    "#discriminator_model.add(Dropout(0.7))\n",
    "#discriminator_model.add(Dense(len(chars), activation='relu'))\n",
    "#discriminator_model.add(LSTM(128, return_sequences=False))\n",
    "#discriminator_model.add(Flatten())\n",
    "discriminator_model.add(Dense(1, activation='sigmoid'))\n",
    "#discriminator_model.add(Dense(1024))\n",
    "#discriminator_model.add(LeakyReLU(0.2))\n",
    "#discriminator_model.add(Dense(512))\n",
    "#discriminator_model.add(LeakyReLU(0.2))\n",
    "#discriminator_model.add(Dropout(0.3))\n",
    "#discriminator_model.add(Dense(1))\n",
    "\n",
    "#discriminator_model = keras.models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Define some layers here..\n",
    "#inputs = keras.layers.Input(shape=(40, len(chars)))\n",
    "\n",
    "#discriminator_model = Sequential()\n",
    "#discriminator_model.add(LSTM(256, input_shape=(40, 59)))\n",
    "#discriminator_model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# Use your layers to create the model.\n",
    "opt = RMSprop(lr=0.001)\n",
    "\n",
    "# Setup the optimisation strategy.\n",
    "discriminator_model.compile(optimizer=opt,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "                             \n",
    "print('compiled.')\n",
    "discriminator_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4020 samples, validate on 1980 samples\n",
      "Epoch 1/10\n",
      "4020/4020 [==============================] - 16s 4ms/step - loss: 0.9314 - acc: 0.5510 - val_loss: 0.6583 - val_acc: 0.6222\n",
      "Epoch 2/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.6909 - acc: 0.5831 - val_loss: 0.6521 - val_acc: 0.5884\n",
      "Epoch 3/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.6908 - acc: 0.5918 - val_loss: 0.7391 - val_acc: 0.4879\n",
      "Epoch 4/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.5989 - acc: 0.6851 - val_loss: 0.4811 - val_acc: 0.7944\n",
      "Epoch 5/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.3965 - acc: 0.8261 - val_loss: 0.1899 - val_acc: 0.9303\n",
      "Epoch 6/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.2776 - acc: 0.8796 - val_loss: 0.1465 - val_acc: 0.9470\n",
      "Epoch 7/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.2139 - acc: 0.9107 - val_loss: 0.1199 - val_acc: 0.9591\n",
      "Epoch 8/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.2049 - acc: 0.9169 - val_loss: 0.1333 - val_acc: 0.9495\n",
      "Epoch 9/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.1693 - acc: 0.9338 - val_loss: 0.1201 - val_acc: 0.9545\n",
      "Epoch 10/10\n",
      "4020/4020 [==============================] - 7s 2ms/step - loss: 0.1473 - acc: 0.9403 - val_loss: 0.1037 - val_acc: 0.9662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d170c8ef0>"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_train, x_test, y_train, y_test] = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "discriminator_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 40, 57)\n",
      "(4020, 1)\n",
      "(1980, 40, 57)\n",
      "(1980, 1)\n"
     ]
    }
   ],
   "source": [
    "# Once you're happy with your discriminator model, evaluate this cell to save it:\n",
    "save_model(discriminator_model, './discriminator_model.h5')\n",
    "# Run these commands in the terminal to submit your model for assessment.\n",
    "# git add lab-07/discriminator_model.h5\n",
    "# git commit -m \"Add/update discriminator model.\"\n",
    "# git push\n",
    "# submit-lab 7\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.10\n",
      "Validation Accuracy: 96.62%\n"
     ]
    }
   ],
   "source": [
    "score,acc = discriminator_model.evaluate(x_test, y_test, verbose = 2, batch_size = 64)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Validation Accuracy: %.2f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
