{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lab 7 - Text generation with LSTM\n",
    "#\n",
    "# Step 1 (not assessed): build and train a model to generate text in the style of a corpus.\n",
    "#\n",
    "# Based on the Keras text generation example (https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)\n",
    "#\n",
    "# Step 2: build a model to distinguish genuine from fake sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential modules\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import save_model\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to sample an index from an array of predictions.\n",
    "#\n",
    "# The input array 'preds' should be the output of a text generation model.\n",
    "# The elements contain the values of the units in the final layer.\n",
    "# Each unit corresponds to a character in the text alphabet.\n",
    "# The final layer should have SoftMax activation, and thus the\n",
    "# value corresponds to the 'strength of prediction' of that character\n",
    "# as the next output value---so the maximum value indicates which character\n",
    "# is most strongly predicted (considerd most likely) as the next one.\n",
    "#\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Convert to high-precision datatype (we are going to be manipulating some\n",
    "    # very small values in this function)\n",
    "    preds = np.asarray(preds).astype('float64')  \n",
    "    \n",
    "    # The next line has the effect of raising each prediction value to the power 1/T.\n",
    "    # It's done using logs to improve numerical precision.  This is a kind of value-dependent\n",
    "    # scaling: for T < 1.0 (1/T > 1.0), small values are made smaller (proportionally) than \n",
    "    # large values (unlike a linear scaling, such as multiplication by 0.9, which scales all values\n",
    "    # the same).\n",
    "    #\n",
    "    # Example: Consider that we have only two symbols (letters) in our alphabet, and our \n",
    "    # probabilities are [0.2, 0.8].  A temperature of 1.0 means 'do not adjust the\n",
    "    # probabilities at all', so in this case there will be a 20% chance that the \n",
    "    # function will return 'symbol 0' and an 80% chance  that it will return 'symbol 1'.\n",
    "    # Note that symbol 1 is 4x more likely than symbol 0.\n",
    "    #\n",
    "    # Now: if we supply a temperature of 0.5, our probabilites will be raised to the\n",
    "    # power 1/0.5 = 2, becoming [0.04, 0.64].  These will then be normalized to sum to 1,\n",
    "    # but anyway it is clear that symbol 1 is here 16x (the square of 4x) more likely than \n",
    "    # symbol 0.\n",
    "    #\n",
    "    # Conversely, for a temperature of 2, our probabilities will be raised to 0.5 (square-rooted),\n",
    "    # becoming [.4472, 0.8944] - and so here symbol 1 is only 2x (sqrt of 4x) more likely than\n",
    "    # symbol 0.\n",
    "    #\n",
    "    # So: low temperatures make the distribution peakier, exaggerating the difference between\n",
    "    # values.  High temperatures flatten the distribution, reducing the difference between values.\n",
    "    #\n",
    "    # As the return value is a sample of the manipulated distribution, manipulating it to\n",
    "    # be peakier (by supplying a low temperature) makes the sample more conservative, i.e.\n",
    "    # more likely to pick the highest-probability symbol.\n",
    "    #\n",
    "    # Making the distribution flatter (by suppyling a high temperature) causes the\n",
    "    # sample to be less conservative, i.e. more likely to pick some lower-likelihood\n",
    "    # symbol.\n",
    "    #\n",
    "    # Phew!\n",
    "    preds = np.exp(np.log(preds) / temperature)\n",
    "    \n",
    "    preds = preds / np.sum(preds)  # ensure that probs sum to 1\n",
    "    probas = np.random.multinomial(1, preds, 1)  # take 1 sample from the distribution\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how much data to use for training.\n",
    "# You might want to reduce this to ~100k for faster experimentation, and then bring it back\n",
    "# to 600k when you're happy with your network architecture.\n",
    "# IMPORTANT: mke sure you end up with a 57-symbol alphabet after reducing the corpus size!\n",
    "# If the number of symbols (shown in the next cell) gets smaller than it was with the full\n",
    "# corpus, bring your sample size back up.  This is necessary because the encoding used for\n",
    "# training must match that used for assessment.\n",
    "desired_num_chars = 600*1000  # Max: 600893\n",
    "\n",
    "random.seed(43)  # Fix random seed for repeatable results.\n",
    "\n",
    "# Slurp down all of Nietzsche from Amazon.\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('original corpus length:', len(text))\n",
    "\n",
    "start_index = random.randint(0, len(text) - desired_num_chars - 1)\n",
    "text = text[start_index:start_index + desired_num_chars]\n",
    "text\n",
    "print('length for training:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a quick look at a random exceprt.\n",
    "#\n",
    "# Caution: Nietzsche might drive you mad: dare you behold more than 1000 of his terrible chars..? \n",
    "sample_length = 1000\n",
    "\n",
    "random.seed(None)  # Seeds random from current time (so re-eval this cell for a new sample).\n",
    "\n",
    "start_index = random.randint(0, len(text) - sample_length - 1)\n",
    "print(text[start_index:start_index+sample_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the alphabet (set of symbols) we are going to use.\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "print(chars)\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))  # Map to look up index of a particular char (e.g. x['a'] = 0)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))  # Map to look up char at an index (e.g. x[0] = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a training set of semi-redundant (i.e. overlapping) sequences of maxlen characters.\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []  # Not syntactic sentences, but just sequences of 40 chars pulled from the corpus.\n",
    "next_chars = [] # next_chars[n] stores the character which followed sentences[n]\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to one-hot encoding.\n",
    "# 'x' will contain the one-hot encoding of the training 'sentences'.\n",
    "# 'y' will contain the one-hot encoding of the 'next char' for each sentence.\n",
    "#\n",
    "# \n",
    "# Let's consider that we have N sentences of length L:\n",
    "#\n",
    "# The 'native' encoding is an NxL matrix where element [n][l]\n",
    "# is the symbol index for character at index (l) of sentence (n)\n",
    "# (e.g., say, 5, corresponding to 'e').\n",
    "#\n",
    "# The one-hot encoding is an NxLxS matrix, where S is the \n",
    "# number of symbols in the alphabet, such that element [n][l][s]\n",
    "# is 1 if the character at index (l) in sentence (n) has the\n",
    "# symbol index (s), and 0 otherwise.\n",
    "def onehot_encode(sentence, maxlen):\n",
    "    x = np.zeros((maxlen, len(chars)), dtype=np.bool)\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[t, char_indices[char]] = 1\n",
    "    return x\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x[i,:,:] = onehot_encode(sentence, maxlen)\n",
    "    y[i, :] = onehot_encode(next_chars[i], 1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the generator model: a single GRU layer with 128 cells.\n",
    "generator_model = Sequential()\n",
    "generator_model.add(GRU(128, input_shape=(maxlen, len(chars))))\n",
    "generator_model.add(Dense(len(chars)))\n",
    "generator_model.add(Activation('softmax'))\n",
    "\n",
    "# You could experiment with NAdam instead of RMSProp.\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "generator_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "trained_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence_list(seed_list, length=400, temperature=0.25):\n",
    "    sentence_list = [];\n",
    "    generated_list = [];\n",
    "    n = len(seed_list)\n",
    "    # copy lists\n",
    "    for seed in seed_list:\n",
    "        sentence_list.append(seed[:])\n",
    "        generated_list.append(seed[:])    \n",
    "    \n",
    "    for i in range(length):\n",
    "      \n",
    "        workdone = (i+1)*1.0 / length\n",
    "        sys.stdout.write(\"\\rgenerating sentences: [{0:20s}] {1:.1f}%\".format('#' * int(workdone * 20), workdone*100))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        x_pred_list = np.zeros((n, maxlen, len(chars)))\n",
    "        for j, sentence in enumerate(sentence_list):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred_list[j, t, char_indices[char]] = 1.\n",
    "\n",
    "        start = time.time()\n",
    "        pred_list = generator_model.predict(x_pred_list, verbose=0)\n",
    "        end = time.time()\n",
    "\n",
    "        for j in range(n):\n",
    "            next_index = sample(pred_list[j,:], temperature)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated_list[j] += next_char\n",
    "            sentence_list[j] = sentence_list[j][1:] + next_char\n",
    "    \n",
    "    sys.stdout.write(' - done\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return generated_list\n",
    "\n",
    "def print_sentences(seeds, sentences):\n",
    "    for seed, sentence in zip(seeds, sentences):\n",
    "        print('-'*5)\n",
    "        sys.stdout.write('\\x1b[32m')\n",
    "        sys.stdout.write(sentence[0:len(seed)])\n",
    "        sys.stdout.write('\\x1b[34m')\n",
    "        sys.stdout.write(sentence[len(seed):-1])\n",
    "        sys.stdout.write('\\x1b[m')\n",
    "        sys.stdout.write('\\n')    \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def pick_sentences(n, maxlen):\n",
    "    global text    \n",
    "    start_index_list = np.random.randint(len(text) - maxlen - 1, size=(1, n)).flatten().tolist()\n",
    "    seed_list = [] \n",
    "    for start_index in start_index_list:\n",
    "        seed_list.append(text[start_index: start_index + maxlen])\n",
    "    return seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate 3 seeds which we will use to inspect the progress of our training:\n",
    "preview_seeds = pick_sentences(3, maxlen=40)\n",
    "\n",
    "# Train the model, output generated text after each iteration\n",
    "for iteration in range(1, 10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    generator_model.fit(x, y,\n",
    "                  batch_size=1024,\n",
    "                  epochs=4)\n",
    "\n",
    "    generated_sentences = generate_sentence_list(preview_seeds)\n",
    "    print_sentences(preview_seeds, generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more complete inspection, print out a load of sentences:\n",
    "#\n",
    "num_sentences = 100             # how many to generate\n",
    "sentence_length = 40            # 100--400 is good\n",
    "sample_temperature = 0.25       # see discussion of temperature up near the top\n",
    "\n",
    "start_index_list = np.random.randint(len(text) - maxlen - 1, size=(1, num_sentences)).flatten().tolist()\n",
    "preview_seeds = [] \n",
    "for start_index in start_index_list:\n",
    "    preview_seeds.append(text[start_index: start_index + maxlen])\n",
    "\n",
    "generated_sentences = generate_sentence_list(preview_seeds, length=sentence_length, temperature=sample_temperature); \n",
    "print_sentences(preview_seeds, generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is just a checkpoint, which will let you download and re-upload (or add to git) this model.\n",
    "save_model(generator_model, './generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the training fake sentences for the Discriminator network\n",
    "#\n",
    "# These are saved to the file 'fake.pkl' -- you could download this to your\n",
    "# user drive and re-upload it in a subsequent session, to save regenerating\n",
    "# it again (in which case you don't need to evaluate this cell).\n",
    "\n",
    "training_seeds = pick_sentences(3000, maxlen=40)\n",
    "training_generated_sentences = generate_sentence_list(training_seeds, length=40)\n",
    "# Strip out the initial 40 chars (the seed sequence, which is genuine data from the corpus).\n",
    "for i, sentence in enumerate(training_generated_sentences):\n",
    "    training_generated_sentences[i] = sentence[40:40+40]\n",
    "    \n",
    "output = open('fake.pkl', 'wb')\n",
    "pickle.dump(training_seeds, output)\n",
    "pickle.dump(training_generated_sentences, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training set from the file\n",
    "pkl_file = open('fake.pkl', 'rb')\n",
    "training_seeds = pickle.load(pkl_file)\n",
    "training_generated_sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a 50:50 set of 'fake' (generated) and genuine sentences:\n",
    "num_generated = len(training_generated_sentences)\n",
    "training_real_sentences = pick_sentences(num_generated, maxlen=40)\n",
    "\n",
    "all_training_sentences = training_generated_sentences + training_real_sentences\n",
    "n = len(all_training_sentences)\n",
    "x = np.zeros((n, 40, len(chars)))\n",
    "y = np.zeros((n, 1))\n",
    "\n",
    "for i, sentence in enumerate(all_training_sentences):\n",
    "    x[i, :, :] = onehot_encode(sentence, maxlen=40)\n",
    "y[num_generated:] = 1  # Encodes the fact that sentences with indexes larger than (num_generated) are real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "# Define some layers here..\n",
    "\n",
    "# Use your layers to create the model.\n",
    "discriminator_model = # ...\n",
    "opt = RMSprop(lr=0.01)\n",
    "\n",
    "# Setup the optimisation strategy.\n",
    "discriminator_model.compile(optimizer=opt,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "                             \n",
    "print('compiled.')\n",
    "discriminator_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x_train, x_test, y_train, y_test] = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "discriminator_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you're happy with your discriminator model, evaluate this cell to save it:\n",
    "save_model(discriminator_model, './discriminator_model.h5')\n",
    "# Run these commands in the terminal to submit your model for assessment.\n",
    "# git add lab-07/discriminator_model.h5\n",
    "# git commit -m \"Add/update discriminator model.\"\n",
    "# git push\n",
    "# submit-lab 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
