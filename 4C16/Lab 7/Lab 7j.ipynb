{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 - Text generation with LSTM\n",
    "#\n",
    "# Step 1 (not assessed): build and train a model to generate text in the style of a corpus.\n",
    "#\n",
    "# Based on the Keras text generation example (https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py)\n",
    "#\n",
    "# Step 2: build a model to distinguish genuine from fake sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential modules\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Activation, Conv1D, Dropout, Flatten\n",
    "from keras.layers import CuDNNGRU, CuDNNLSTM, GaussianNoise, BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import RMSprop, Adam, Nadam, SGD\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import save_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import initializers\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample an index from an array of predictions.\n",
    "#\n",
    "# The input array 'preds' should be the output of a text generation model.\n",
    "# The elements contain the values of the units in the final layer.\n",
    "# Each unit corresponds to a character in the text alphabet.\n",
    "# The final layer should have SoftMax activation, and thus the\n",
    "# value corresponds to the 'strength of prediction' of that character\n",
    "# as the next output value---so the maximum value indicates which character\n",
    "# is most strongly predicted (considerd most likely) as the next one.\n",
    "#\n",
    "def sample(preds, temperature=1.0):\n",
    "    # Convert to high-precision datatype (we are going to be manipulating some\n",
    "    # very small values in this function)\n",
    "    preds = np.asarray(preds).astype('float64')  \n",
    "    \n",
    "    # The next line has the effect of raising each prediction value to the power 1/T.\n",
    "    # It's done using logs to improve numerical precision.  This is a kind of value-dependent\n",
    "    # scaling: for T < 1.0 (1/T > 1.0), small values are made smaller (proportionally) than \n",
    "    # large values (unlike a linear scaling, such as multiplication by 0.9, which scales all values\n",
    "    # the same).\n",
    "    #\n",
    "    # Example: Consider that we have only two symbols (letters) in our alphabet, and our \n",
    "    # probabilities are [0.2, 0.8].  A temperature of 1.0 means 'do not adjust the\n",
    "    # probabilities at all', so in this case there will be a 20% chance that the \n",
    "    # function will return 'symbol 0' and an 80% chance  that it will return 'symbol 1'.\n",
    "    # Note that symbol 1 is 4x more likely than symbol 0.\n",
    "    #\n",
    "    # Now: if we supply a temperature of 0.5, our probabilites will be raised to the\n",
    "    # power 1/0.5 = 2, becoming [0.04, 0.64].  These will then be normalized to sum to 1,\n",
    "    # but anyway it is clear that symbol 1 is here 16x (the square of 4x) more likely than \n",
    "    # symbol 0.\n",
    "    #\n",
    "    # Conversely, for a temperature of 2, our probabilities will be raised to 0.5 (square-rooted),\n",
    "    # becoming [.4472, 0.8944] - and so here symbol 1 is only 2x (sqrt of 4x) more likely than\n",
    "    # symbol 0.\n",
    "    #\n",
    "    # So: low temperatures make the distribution peakier, exaggerating the difference between\n",
    "    # values.  High temperatures flatten the distribution, reducing the difference between values.\n",
    "    #\n",
    "    # As the return value is a sample of the manipulated distribution, manipulating it to\n",
    "    # be peakier (by supplying a low temperature) makes the sample more conservative, i.e.\n",
    "    # more likely to pick the highest-probability symbol.\n",
    "    #\n",
    "    # Making the distribution flatter (by suppyling a high temperature) causes the\n",
    "    # sample to be less conservative, i.e. more likely to pick some lower-likelihood\n",
    "    # symbol.\n",
    "    #\n",
    "    # Phew!\n",
    "    preds = np.exp(np.log(preds) / temperature)\n",
    "    \n",
    "    preds = preds / np.sum(preds)  # ensure that probs sum to 1\n",
    "    probas = np.random.multinomial(1, preds, 1)  # take 1 sample from the distribution\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original corpus length: 600901\n",
      "length for training: 480139\n",
      "char count 57\n"
     ]
    }
   ],
   "source": [
    "# Decide how much data to use for training.\n",
    "# You might want to reduce this to ~100k for faster experimentation, and then bring it back\n",
    "# to 600k when you're happy with your network architecture.\n",
    "# IMPORTANT: mke sure you end up with a 57-symbol alphabet after reducing the corpus size!\n",
    "# If the number of symbols (shown in the next cell) gets smaller than it was with the full\n",
    "# corpus, bring your sample size back up.  This is necessary because the encoding used for\n",
    "# training must match that used for assessment.\n",
    "#desired_num_chars = 600*1000  # Max: 600893\n",
    "desired_num_chars = 480139  # Max: 600893\n",
    "\n",
    "random.seed(43)  # Fix random seed for repeatable results.\n",
    "\n",
    "# Slurp down all of Nietzsche from Amazon.\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('original corpus length:', len(text))\n",
    "\n",
    "start_index = random.randint(0, len(text) - desired_num_chars - 1)\n",
    "text = text[start_index:start_index + desired_num_chars]\n",
    "text\n",
    "print('length for training:', len(text))\n",
    "num_chars = len(sorted(list(set(text))))\n",
    "print('char count', num_chars)\n",
    "\n",
    "#for i in range (-100000, 100000):\n",
    "#    random.seed(i)\n",
    "#    path_i = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "#    text_i = open(path_i).read().lower()\n",
    "#    start_index_i = random.randint(0, len(text_i) - desired_num_chars - 1)\n",
    "#    text_i = text_i[start_index_i:start_index_i + desired_num_chars]\n",
    "#    num_chars_i = len(sorted(list(set(text_i))))\n",
    "#    if num_chars_i == 57: break\n",
    "\n",
    "#print('i', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oved, but does not itself love,\n",
      "betrays its sediment: its dregs come up.\n",
      "\n",
      "80. a thing that is explained ceases to concern us--what did the god\n",
      "mean who gave the advice, \"know thyself!\" did it perhaps imply \"cease to\n",
      "be concerned about thyself! become objective!\"--and socrates?--and the\n",
      "\"scientific man\"?\n",
      "\n",
      "81. it is terrible to die of thirst at sea. is it necessary that you\n",
      "should so salt your truth that it will no longer--quench thirst?\n",
      "\n",
      "82. \"sympathy for all\"--would be harshness and tyranny for thee, my good\n",
      "neighbour.\n",
      "\n",
      "83. instinct--when the house is on fire one forgets even the\n",
      "dinner--yes, but one recovers it from among the ashes.\n",
      "\n",
      "84. woman learns how to hate in proportion as she--forgets how to charm.\n",
      "\n",
      "85. the same emotions are in man and woman, but in different tempo, on\n",
      "that account man and woman never cease to misunderstand each other.\n",
      "\n",
      "86. in the background of all their personal vanity, women themselves\n",
      "have still their impersonal scorn--for \"woman\".\n",
      "\n",
      "87. fettered heart, free \n"
     ]
    }
   ],
   "source": [
    "# Let's have a quick look at a random exceprt.\n",
    "#\n",
    "# Caution: Nietzsche might drive you mad: dare you behold more than 1000 of his terrible chars..? \n",
    "sample_length = 1000\n",
    "\n",
    "random.seed(None)  # Seeds random from current time (so re-eval this cell for a new sample).\n",
    "\n",
    "start_index = random.randint(0, len(text) - sample_length - 1)\n",
    "print(text[start_index:start_index+sample_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¤', '¦', '«', 'ã']\n"
     ]
    }
   ],
   "source": [
    "# Establish the alphabet (set of symbols) we are going to use.\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "print(chars)\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))  # Map to look up index of a particular char (e.g. x['a'] = 0)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))  # Map to look up char at an index (e.g. x[0] = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 160033\n"
     ]
    }
   ],
   "source": [
    "# Establish a training set of semi-redundant (i.e. overlapping) sequences of maxlen characters.\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []  # Not syntactic sentences, but just sequences of 40 chars pulled from the corpus.\n",
    "next_chars = [] # next_chars[n] stores the character which followed sentences[n]\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160033, 40, 57)\n",
      "(160033, 57)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data to one-hot encoding.\n",
    "# 'x' will contain the one-hot encoding of the training 'sentences'.\n",
    "# 'y' will contain the one-hot encoding of the 'next char' for each sentence.\n",
    "#\n",
    "# \n",
    "# Let's consider that we have N sentences of length L:\n",
    "#\n",
    "# The 'native' encoding is an NxL matrix where element [n][l]\n",
    "# is the symbol index for character at index (l) of sentence (n)\n",
    "# (e.g., say, 5, corresponding to 'e').\n",
    "#\n",
    "# The one-hot encoding is an NxLxS matrix, where S is the \n",
    "# number of symbols in the alphabet, such that element [n][l][s]\n",
    "# is 1 if the character at index (l) in sentence (n) has the\n",
    "# symbol index (s), and 0 otherwise.\n",
    "def onehot_encode(sentence, maxlen):\n",
    "    x = np.zeros((maxlen, len(chars)), dtype=np.bool)\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[t, char_indices[char]] = 1\n",
    "    return x\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    x[i,:,:] = onehot_encode(sentence, maxlen)\n",
    "    y[i, :] = onehot_encode(next_chars[i], 1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the generator model: a single GRU layer with 128 cells.\n",
    "#generator_model = Sequential()\n",
    "#generator_model.add(GRU(128, input_shape=(maxlen, len(chars))))\n",
    "#generator_model.add(Dense(len(chars)))\n",
    "#generator_model.add(Activation('softmax'))\n",
    "\n",
    "# You could experiment with NAdam instead of RMSProp.\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "#generator_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#trained_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru_7 (CuDNNGRU)       (None, 40, 64)            23616     \n",
      "_________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNo (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_8 (CuDNNGRU)       (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 57)                7353      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 57)                0         \n",
      "=================================================================\n",
      "Total params: 105,465\n",
      "Trainable params: 105,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#GRU generator\n",
    "gru_generator_model = Sequential()\n",
    "gru_generator_model.add(CuDNNGRU(64, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "gru_generator_model.add(GaussianNoise(1))\n",
    "gru_generator_model.add(CuDNNGRU(128, return_sequences=False, input_shape=(maxlen, len(chars))))\n",
    "gru_generator_model.add(Dense(len(chars)))\n",
    "gru_generator_model.add(Activation('softmax'))\n",
    "\n",
    "# You could experiment with NAdam instead of RMSProp.\n",
    "gru_generator_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "gru_trained_epochs = 0\n",
    "gru_generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               95232     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 57)                7353      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 57)                0         \n",
      "=================================================================\n",
      "Total params: 102,585\n",
      "Trainable params: 102,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM generator\n",
    "lstm_generator_model = Sequential()\n",
    "lstm_generator_model.add(LSTM(128, return_sequences=False, input_shape=(maxlen, len(chars))))\n",
    "lstm_generator_model.add(Dense(len(chars)))\n",
    "lstm_generator_model.add(Activation('softmax'))\n",
    "\n",
    "# You could experiment with NdaAm instead of RMSProp.\n",
    "#optimizer = Nadam(lr=0.005)\n",
    "lstm_generator_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "lstm_trained_epochs = 0\n",
    "lstm_generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_list(seed_list, length=400, temperature=0.25, model_type='gru'):\n",
    "    sentence_list_1 = [];\n",
    "    generated_list_1 = [];\n",
    "    n = len(seed_list)\n",
    "    # copy lists\n",
    "    temperature_1 = temperature\n",
    "    for seed in seed_list:\n",
    "        sentence_list_1.append(seed[:])\n",
    "        generated_list_1.append(seed[:])\n",
    "    \n",
    "    for i in range(length):\n",
    "      \n",
    "        workdone = (i+1)*1.0 / length\n",
    "        sys.stdout.write(\"\\rgenerating {0:} sentences: [{1:20s}] {2:.1f}%\".format(model_type,\n",
    "                                                                                  '#' * int(workdone * 20), workdone*100))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        x_pred_list = np.zeros((n, maxlen, len(chars)))\n",
    "        for j, sentence in enumerate(sentence_list_1):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred_list[j, t, char_indices[char]] = 1.\n",
    "\n",
    "        start = time.time()\n",
    "        if (model_type == 'lstm'):\n",
    "            pred_list = lstm_generator_model.predict(x_pred_list, verbose=0)\n",
    "        else:\n",
    "            pred_list = gru_generator_model.predict(x_pred_list, verbose=0)\n",
    "        end = time.time()\n",
    "\n",
    "        for j in range(n):\n",
    "            next_index_1 = sample(pred_list[j,:], temperature_1)\n",
    "            next_char_1 = indices_char[next_index_1]\n",
    "            generated_list_1[j] += next_char_1\n",
    "            sentence_list_1[j] = sentence_list_1[j][1:] + next_char_1\n",
    "    \n",
    "    sys.stdout.write(' - done\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    generated_list = generated_list_1\n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_list_n(seed_list, length=400, temperature=0.25, model_type='gru'):\n",
    "    sentence_list_1 = [];\n",
    "    sentence_list_2 = [];\n",
    "    sentence_list_3 = [];\n",
    "    generated_list_1 = [];\n",
    "    generated_list_2 = [];\n",
    "    generated_list_3 = [];\n",
    "    n = len(seed_list)\n",
    "    # copy lists\n",
    "    temperature_1 = temperature\n",
    "    temperature_2 = (temperature + 0.15)\n",
    "    temperature_3 = max(0.01, (temperature - 0.15))\n",
    "    for seed in seed_list:\n",
    "        sentence_list_1.append(seed[:])\n",
    "        sentence_list_2.append(seed[:])\n",
    "        sentence_list_3.append(seed[:])\n",
    "        generated_list_1.append(seed[:])\n",
    "        generated_list_2.append(seed[:]) \n",
    "        generated_list_3.append(seed[:]) \n",
    "    \n",
    "    for i in range(length):\n",
    "      \n",
    "        workdone = (i+1)*1.0 / length\n",
    "        sys.stdout.write(\"\\rgenerating {0:} sentences: [{1:20s}] {2:.1f}%\"\n",
    "                         .format(model_type, '#' * int(workdone * 20), workdone*100))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        x_pred_list = np.zeros((n, maxlen, len(chars)))\n",
    "        for j, sentence in enumerate(sentence_list_1):\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred_list[j, t, char_indices[char]] = 1.\n",
    "\n",
    "        start = time.time()\n",
    "        if (model_type == 'lstm'):\n",
    "            pred_list = lstm_generator_model.predict(x_pred_list, verbose=0)\n",
    "        else:\n",
    "            pred_list = gru_generator_model.predict(x_pred_list, verbose=0)\n",
    "        end = time.time()\n",
    "\n",
    "        for j in range(n):\n",
    "            next_index_1 = sample(pred_list[j,:], temperature_1)\n",
    "            next_char_1 = indices_char[next_index_1]\n",
    "            generated_list_1[j] += next_char_1\n",
    "            sentence_list_1[j] = sentence_list_1[j][1:] + next_char_1\n",
    "            next_index_2 = sample(pred_list[j,:], temperature_2)\n",
    "            next_char_2 = indices_char[next_index_2]\n",
    "            generated_list_2[j] += next_char_2\n",
    "            sentence_list_2[j] = sentence_list_2[j][1:] + next_char_2\n",
    "            next_index_3 = sample(pred_list[j,:], temperature_3)\n",
    "            next_char_3 = indices_char[next_index_3]\n",
    "            generated_list_3[j] += next_char_3\n",
    "            sentence_list_3[j] = sentence_list_3[j][1:] + next_char_3\n",
    "    \n",
    "    sys.stdout.write(' - done\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    generated_list = generated_list_1 + generated_list_2 + generated_list_3\n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentences(seeds, sentences):\n",
    "    for seed, sentence in zip(seeds, sentences):\n",
    "        print('-'*5)\n",
    "        sys.stdout.write('\\x1b[32m')\n",
    "        sys.stdout.write(sentence[0:len(seed)])\n",
    "        sys.stdout.write('\\x1b[34m')\n",
    "        sys.stdout.write(sentence[len(seed):-1])\n",
    "        sys.stdout.write('\\x1b[m')\n",
    "        sys.stdout.write('\\n')    \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def pick_sentences(n, maxlen):\n",
    "    global text    \n",
    "    start_index_list = np.random.randint(len(text) - maxlen - 1, size=(1, n)).flatten().tolist()\n",
    "    seed_list = [] \n",
    "    for start_index in start_index_list:\n",
    "        seed_list.append(text[start_index: start_index + maxlen])\n",
    "    return seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 12s 72us/step - loss: 3.1469 - acc: 0.1372 2s - loss: 3.1736\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 2.9782 - acc: 0.1624\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 2.7344 - acc: 0.2342\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 2.5600 - acc: 0.2697\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 19s 120us/step - loss: 3.0622 - acc: 0.1539\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 19s 116us/step - loss: 2.7298 - acc: 0.2499\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 19s 119us/step - loss: 2.5028 - acc: 0.2935\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 19s 121us/step - loss: 2.3988 - acc: 0.3136\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 6s 38us/step - loss: 2.4473 - acc: 0.2935\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 6s 37us/step - loss: 2.3677 - acc: 0.3138\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 2.2971 - acc: 0.3317\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 2.2421 - acc: 0.3436\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 19s 118us/step - loss: 2.3227 - acc: 0.3318\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 19s 121us/step - loss: 2.2623 - acc: 0.3440\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 19s 117us/step - loss: 2.2148 - acc: 0.3543\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 20s 124us/step - loss: 2.1744 - acc: 0.3637\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 2.1925 - acc: 0.3575: 3s - loss -\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 2.1481 - acc: 0.3690\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 2.1101 - acc: 0.3787: 3s - loss: 2.1\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 2.0745 - acc: 0.3893: 3s - - \n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 18s 111us/step - loss: 2.1382 - acc: 0.3737\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 20s 124us/step - loss: 2.1051 - acc: 0.38221s - loss: 2.1051 - \n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 20s 126us/step - loss: 2.0790 - acc: 0.3891\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 27s 171us/step - loss: 2.0497 - acc: 0.3970\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 6s 36us/step - loss: 2.0442 - acc: 0.3975\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 2.0159 - acc: 0.4058: 0s - loss: 2.0176 - a\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 30us/step - loss: 1.9915 - acc: 0.4125: - ETA: 0s - loss: 1.9966 - acc: 0.4 - ETA: 0s - loss: 1.9962 \n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 30us/step - loss: 1.9683 - acc: 0.4177: 1s - loss: 1.9729 - acc: 0 - ETA: 1s - l\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 18s 114us/step - loss: 2.0234 - acc: 0.4039\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 18s 111us/step - loss: 1.9988 - acc: 0.4112\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 19s 117us/step - loss: 1.9753 - acc: 0.4170\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 18s 115us/step - loss: 1.9539 - acc: 0.4239\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 5s 30us/step - loss: 1.9485 - acc: 0.4243\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 1.9279 - acc: 0.4309: 3 - ETA\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 1.9114 - acc: 0.4335\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 1.8948 - acc: 0.4389\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 22s 135us/step - loss: 1.9338 - acc: 0.4300\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 22s 138us/step - loss: 1.9148 - acc: 0.4356\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 19s 119us/step - loss: 1.8970 - acc: 0.4416\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 18s 115us/step - loss: 1.8792 - acc: 0.4456\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 5s 31us/step - loss: 1.8812 - acc: 0.4408: 1s - l\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 30us/step - loss: 1.8656 - acc: 0.4452: 1s - \n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 30us/step - loss: 1.8518 - acc: 0.4499: 0s - loss: 1.8511 - acc: 0\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.8398 - acc: 0.4533: 0s - loss: 1.8400 - acc:\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 22s 138us/step - loss: 1.8626 - acc: 0.4505\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 22s 140us/step - loss: 1.8472 - acc: 0.45471s - loss: 1.8471 - \n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 20s 125us/step - loss: 1.8325 - acc: 0.4575\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 18s 112us/step - loss: 1.8190 - acc: 0.4616\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.8282 - acc: 0.4550: 1\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.8168 - acc: 0.4586\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 33us/step - loss: 1.8058 - acc: 0.4623\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.7999 - acc: 0.4627: 1s - loss: 1.79\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 18s 114us/step - loss: 1.8057 - acc: 0.4658\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 22s 135us/step - loss: 1.7915 - acc: 0.4698\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 19s 117us/step - loss: 1.7797 - acc: 0.4726\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 20s 124us/step - loss: 1.7684 - acc: 0.4769\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.7877 - acc: 0.4676: 0s - loss: 1.7859 - \n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.7810 - acc: 0.4692: 0s - loss: 1.7801 - acc: \n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 33us/step - loss: 1.7718 - acc: 0.4710\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.7642 - acc: 0.4728: 1s - loss: \n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 18s 115us/step - loss: 1.7562 - acc: 0.4804\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 19s 122us/step - loss: 1.7458 - acc: 0.4837\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 19s 118us/step - loss: 1.7346 - acc: 0.4867\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 18s 114us/step - loss: 1.7247 - acc: 0.4896\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 5s 33us/step - loss: 1.7555 - acc: 0.4752\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 5s 32us/step - loss: 1.7491 - acc: 0.4786\n",
      "Epoch 3/4\n",
      "160033/160033 [==============================] - 5s 33us/step - loss: 1.7429 - acc: 0.4797: 1s - \n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 6s 35us/step - loss: 1.7352 - acc: 0.4821\n",
      "Epoch 1/4\n",
      "160033/160033 [==============================] - 20s 124us/step - loss: 1.7156 - acc: 0.4927\n",
      "Epoch 2/4\n",
      "160033/160033 [==============================] - 20s 123us/step - loss: 1.7058 - acc: 0.4951\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160033/160033 [==============================] - 19s 121us/step - loss: 1.6968 - acc: 0.4979\n",
      "Epoch 4/4\n",
      "160033/160033 [==============================] - 19s 117us/step - loss: 1.6886 - acc: 0.5002\n"
     ]
    }
   ],
   "source": [
    "# Generate 3 seeds which we will use to inspect the progress of our training:\n",
    "#preview_seeds = pick_sentences(3, maxlen=40)\n",
    "\n",
    "# Train the model, output generated text after each iteration\n",
    "for iteration in range(1, 10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    print('GRU')\n",
    "    gru_generator_model.fit(x, y,\n",
    "                  batch_size=1024,\n",
    "                  epochs=4)\n",
    "    print('LSTM')\n",
    "    lstm_generator_model.fit(x, y,\n",
    "                  batch_size=1024,\n",
    "                  epochs=4)\n",
    "    \n",
    "    #generated_sentences = generate_sentence_list(preview_seeds)\n",
    "    #print_sentences(preview_seeds, generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 'gru' sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mone. at the same\n",
      "time, it should be furt\u001b[34mure, and the post of the expection of t\u001b[m\n",
      "-----\n",
      "\u001b[32mginning of the revolution,\n",
      "flung away it\u001b[34m is a strong and the most of such a suc\u001b[m\n",
      "-----\n",
      "\u001b[32mand attained through the self-evolving o\u001b[34mf the man of the truence of the wrom th\u001b[m\n",
      "-----\n",
      "\u001b[32mts and ancestors in his\n",
      "constitution, wh\u001b[34mich has the most seem of the suprecions\u001b[m\n",
      "-----\n",
      "\u001b[32mught, is that the individual fixes his m\u001b[34more for the most mann of the serting of\u001b[m\n",
      "-----\n",
      "\u001b[32m the two\n",
      "principal functions of man. to \u001b[34mshould be the should and the still the \u001b[m\n",
      "-----\n",
      "\u001b[32mrecisely by its protean\n",
      "arts that it is \u001b[34ma sense of its sense of the most desire\u001b[m\n",
      "-----\n",
      "\u001b[32ms limits, the range of man's inner exper\u001b[34mitions of the great of the consection o\u001b[m\n",
      "-----\n",
      "\u001b[32m equally so his regained security. such \u001b[34ma man of the great of the interpression\u001b[m\n",
      "-----\n",
      "\u001b[32mng long periods should regulate their\n",
      "co\u001b[34mnsequently with the strong of the sense\u001b[m\n",
      "-----\n",
      "\u001b[32monic\n",
      "nature, which understood much, and \u001b[34mthe despect of the morality of the sens\u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "moment of inexplicable hesitation, like\u001b[34m a profation of the most desility of th\u001b[m\n",
      "-----\n",
      "\u001b[32mphere--of the moral\n",
      "maxim has almost ine\u001b[34mas stranger, in the constinct of the in\u001b[m\n",
      "-----\n",
      "\u001b[32mg of\n",
      "the restlessness, emptiness, and no\u001b[34mt seems the man of a dever of the man o\u001b[m\n",
      "-----\n",
      "\u001b[32m, as cook for thousands of years, have d\u001b[34meveloped the sace of such and such and \u001b[m\n",
      "-----\n",
      "\u001b[32mogical nature. yet were there\n",
      "steps affo\u001b[34mred of something and seems of the stron\u001b[m\n",
      "-----\n",
      "\u001b[32min\n",
      "itself,\" is perhaps the greatest auda\u001b[34mys of the resting of the resined to the\u001b[m\n",
      "-----\n",
      "\u001b[32min one breath. such periods as occur in \u001b[34mthe german and resiness of the spirit o\u001b[m\n",
      "-----\n",
      "\u001b[32meen partially attempted in the following\u001b[34m of the sense of the semple of a perhap\u001b[m\n",
      "-----\n",
      "\u001b[32mcount, as instruments, they are\n",
      "far from\u001b[34m the desting of the desire of the speci\u001b[m\n",
      "-----\n",
      "\u001b[32mand calls itself good, is the instinct o\u001b[34mf the bearn of all the sempation of the\u001b[m\n",
      "-----\n",
      "\u001b[32mes\"; and in the end,\n",
      "in view of all that\u001b[34m the most stromation of man and the mor\u001b[m\n",
      "-----\n",
      "\u001b[32m-i wager he\n",
      "finds nothing!\n",
      "\n",
      "36. supposin\u001b[34mg the experition of the sense of the ma\u001b[m\n",
      "-----\n",
      "\u001b[32med and attracted by the riddles which th\u001b[34me sense of the man of the strong of the\u001b[m\n",
      "-----\n",
      "\u001b[32mpplied\n",
      "to actions; it is a gross mistake\u001b[34m the consequence, and of the most seems\u001b[m\n",
      "-----\n",
      "\u001b[32mn fell,\n",
      "     and unlearned man and god a\u001b[34ms the desting of the strong of mank of \u001b[m\n",
      "-----\n",
      "\u001b[32m the constant out-looking and down-looki\u001b[34mng and contention of mankind the post o\u001b[m\n",
      "-----\n",
      "\u001b[32m to divine and\n",
      "determine what sort of hi\u001b[34mmself and all the most sention of the f\u001b[m\n",
      "-----\n",
      "\u001b[32mnew ethical adjustment, it is then decid\u001b[34me the sense of the into the strong of t\u001b[m\n",
      "-----\n",
      "\u001b[32mabout love:\n",
      "the martyrdom of the most in\u001b[34m the consequently of the man of the old\u001b[m\n",
      "-----\n",
      "\u001b[32m by a distinguished logician, will be il\u001b[34ml the strome and great the great of the\u001b[m\n",
      "-----\n",
      "\u001b[32me of the subtler after-effects of democr\u001b[34ming to the restrung of the sense of the\u001b[m\n",
      "-----\n",
      "\u001b[32med by the super-abundance of power. the\n",
      "\u001b[34mpriles of the same of the sense of the \u001b[m\n",
      "-----\n",
      "\u001b[32m worse.\n",
      "\n",
      "[5] pneumatic is here used in t\u001b[34mhe expection of the seems of the sense \u001b[m\n",
      "-----\n",
      "\u001b[32me strength of a mind might be measured b\u001b[34me the freed of the soul of the formance\u001b[m\n",
      "-----\n",
      "\u001b[32mteems, but only when one\n",
      "esteems equal o\u001b[34mf the seems of the more of the morality\u001b[m\n",
      "-----\n",
      "\u001b[32morigin and history of the so-called mora\u001b[34mlity of the world of the spirity of the\u001b[m\n",
      "-----\n",
      "\u001b[32mity in wrong--i mean to\n",
      "say into general\u001b[34m in the \"more resure of the consequentl\u001b[m\n",
      "-----\n",
      "\u001b[32mly under them. therefore: in sleep and i\u001b[34mn the same to be the most desire of the\u001b[m\n",
      "-----\n",
      "\u001b[32mey evidently appear too rarely, they are\u001b[34m the supperious of every soul, and sens\u001b[m\n",
      "-----\n",
      "\u001b[32mn treat it with moral\n",
      "indignation. enoug\u001b[34mh the stration of the farther of the sa\u001b[m\n",
      "-----\n",
      "\u001b[32me in himself\" the objective man is in tr\u001b[34messelves of the morality of the great o\u001b[m\n",
      "-----\n",
      "\u001b[32mthey are overturned. it is wilfulness an\u001b[34md the present of the consequently of th\u001b[m\n",
      "-----\n",
      "\u001b[32m.\n",
      "\n",
      "\n",
      "\n",
      "chapter iv. apophthegms and interlu\u001b[34msion of the sense of the freedom of the\u001b[m\n",
      "-----\n",
      "\u001b[32mesitation, protractedness, frequent retr\u001b[34mess of the sense of the some spirition \u001b[m\n",
      "-----\n",
      "\u001b[32mt the older psychologists had a\n",
      "merrier \u001b[34mof the experions of the experious of th\u001b[m\n",
      "-----\n",
      "\u001b[32m of a merely\n",
      "moral man\"--it would make t\u001b[34mhe moral spirit of the sense of the con\u001b[m\n",
      "-----\n",
      "\u001b[32me belief that there are\n",
      "like things (gle\u001b[34mat the will the soul of the servans of \u001b[m\n",
      "-----\n",
      "\u001b[32morigin,\n",
      "with regard to the inherited ple\u001b[34mated and consequently and the man, and \u001b[m\n",
      "-----\n",
      "\u001b[32mances to induce perplexity in the mind, \u001b[34mand the sense of the great of the are o\u001b[m\n",
      "-----\n",
      "\u001b[32mr generation in the future. for metaphys\u001b[34mical the man of the world of every pres\u001b[m\n",
      "-----\n",
      "\u001b[32mfficult for\n",
      "a noble man to understand: h\u001b[34me who a man of the sense of the strange\u001b[m\n",
      "-----\n",
      "\u001b[32mtes, upsets the table, shrieks, raves,\n",
      "a\u001b[34mnd the consequently and the will the di\u001b[m\n",
      "-----\n",
      "\u001b[32mng; for instance,\n",
      "virtue, art, music, da\u001b[34mst sees to has his seems and freedom of\u001b[m\n",
      "-----\n",
      "\u001b[32m\"\n",
      "\n",
      "210. supposing, then, that in the pic\u001b[34mt of the man of the more of the strange\u001b[m\n",
      "-----\n",
      "\u001b[32mor me, i\n",
      "now love every fate:--who would\u001b[34m and distrused the consequent of the mo\u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "epochs, when they show themselves infec\u001b[34mtions of the sense of the freedom of th\u001b[m\n",
      "-----\n",
      "\u001b[32many liberties: i said\n",
      "this once before b\u001b[34mecome the most of the man of the serst \u001b[m\n",
      "-----\n",
      "\u001b[32mas not easily a right to it\"--such a phi\u001b[34mlosophy of the his of the more the stro\u001b[m\n",
      "-----\n",
      "\u001b[32mthe dangerous dyspepsia which originates\u001b[34m and all the expection of the sense of \u001b[m\n",
      "-----\n",
      "\u001b[32m79. the consequences of our actions seiz\u001b[34med to the supperious of the man of the \u001b[m\n",
      "-----\n",
      "\u001b[32mneuma being the\n",
      "greek word in the new te\u001b[34mre the werr of the sense of the strong \u001b[m\n",
      "-----\n",
      "\u001b[32mensations,\n",
      "namely, the sensation of the \u001b[34mmost and consequently and in the presen\u001b[m\n",
      "-----\n",
      "\u001b[32mand acts upon other organs thereby. the \u001b[34msame of the great the experious of the \u001b[m\n",
      "-----\n",
      "\u001b[32mwhile it delights--what? and all that is\u001b[34m the good of the and the consequent of \u001b[m\n",
      "-----\n",
      "\u001b[32mfor the morrow and the day after the mor\u001b[34me respect and his of the more with the \u001b[m\n",
      "-----\n",
      "\u001b[32mich, as we have said, france has not gru\u001b[34msity of the most the constince of the m\u001b[m\n",
      "-----\n",
      "\u001b[32mheer exhaustion. yet, in dreams, we all\n",
      "\u001b[34mthe deveration of the will of the world\u001b[m\n",
      "-----\n",
      "\u001b[32mst mind cannot\n",
      "adequately appreciate the\u001b[34m man of the consequent of the present o\u001b[m\n",
      "-----\n",
      "\u001b[32ms of\n",
      "sentiment;--on this point we must n\u001b[34mot the most of the most of the moral th\u001b[m\n",
      "-----\n",
      "\u001b[32mgin in a false explanation of certain hu\u001b[34mman the constion of the stroegh of the \u001b[m\n",
      "-----\n",
      "\u001b[32me lights and colors because the brain, w\u001b[34mhich is a constinct and the old of the \u001b[m\n",
      "-----\n",
      "\u001b[32muperficiality with regard to the\n",
      "origin \u001b[34mof the same of the experions of the con\u001b[m\n",
      "-----\n",
      "\u001b[32mre simply sublimations in which the basi\u001b[34mon of the should beand of the most spec\u001b[m\n",
      "-----\n",
      "\u001b[32mdequately through his\n",
      "religions and arts\u001b[34m the experies of the post still of the \u001b[m\n",
      "-----\n",
      "\u001b[32mythms without dance, which germans call\n",
      "\u001b[34mthe sense of the man of the consequentl\u001b[m\n",
      "-----\n",
      "\u001b[32md commanding, of keeping down and keepin\u001b[34mg in the san of the sense of the man of\u001b[m\n",
      "-----\n",
      "\u001b[32mde--namely,\n",
      "intellect. one of the subtle\u001b[34m of the sense of the morality of the st\u001b[m\n",
      "-----\n",
      "\u001b[32msity for training the feelings to severi\u001b[34mty of the mist and the sense of the sci\u001b[m\n",
      "-----\n",
      "\u001b[32monditions precedent, as thus\n",
      "conjectured\u001b[34m and the man of the sense of the morali\u001b[m\n",
      "-----\n",
      "\u001b[32mring\n",
      "to light! woman has so much cause f\u001b[34mor the most sense of the man of the sen\u001b[m\n",
      "-----\n",
      "\u001b[32mture--when such a man has sympathy, well\u001b[34m a for the stranger of the will the sen\u001b[m\n",
      "-----\n",
      "\u001b[32mlax, of course, with consideration, and\n",
      "\u001b[34mthe great sense of the expection of the\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "\u001b[32m thing as moral phenomena, but only a mo\u001b[34mre from the resting of the man and supp\u001b[m\n",
      "-----\n",
      "\u001b[32mhe god, devil,\n",
      "sheep, and worm in us, in\u001b[34m the experies of the strogation of the \u001b[m\n",
      "-----\n",
      "\u001b[32mpathy for you!--to be sure, that is not \u001b[34mthe great of the constinct of the old r\u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "how comes it that the mind of the dream\u001b[34m of the some the constinct of the most \u001b[m\n",
      "-----\n",
      "\u001b[32mn if the question whether anything metap\u001b[34mhers of the same of the good of the not\u001b[m\n",
      "-----\n",
      "\u001b[32mrejudiced and almost\n",
      "hostile, is precise\u001b[34md and the present and the sense of the \u001b[m\n",
      "-----\n",
      "\u001b[32mts, they take control of the needy as a\n",
      "\u001b[34mmore the world of the sense of the pres\u001b[m\n",
      "-----\n",
      "\u001b[32mhat\n",
      "psychological observation is one of \u001b[34mthe will the man of the morality of the\u001b[m\n",
      "-----\n",
      "\u001b[32mer the raffinements of\n",
      "decadence--which,\u001b[34m he as a man of the sense of the morali\u001b[m\n",
      "-----\n",
      "\u001b[32mhending subject\n",
      "consists in the inner ne\u001b[34mture of the most and the freedom of the\u001b[m\n",
      "-----\n",
      "\u001b[32m is, indeed, still in\n",
      "process of evoluti\u001b[34mon of the great of the expection of the\u001b[m\n",
      "-----\n",
      "\u001b[32mre,\n",
      "a dissatisfaction with its own condi\u001b[34mcal in the prestion of the sense of the\u001b[m\n",
      "-----\n",
      "\u001b[32m, necessarily doubts the\n",
      "value of life; \u001b[34mand the strong of the consequent of a c\u001b[m\n",
      "-----\n",
      "\u001b[32m individual, into the neighbour and\n",
      "frie\u001b[34mnce of the strong of the sense of every\u001b[m\n",
      "-----\n",
      "\u001b[32mresumable that the objects of the\n",
      "religi\u001b[34mons of the destimation of the moral to \u001b[m\n",
      "-----\n",
      "\u001b[32mficultly\n",
      "understood\" myself!)--and one s\u001b[34mense of the perhaps and present and con\u001b[m\n",
      "-----\n",
      "\u001b[32min the same way as disrespectfulness to\n",
      "\u001b[34mthe strogation of a man and the strong \u001b[m\n",
      "generating 'gru' sentences: [####################] 100.0% - done\n",
      "-----\n",
      "\u001b[32mone. at the same\n",
      "time, it should be furt\u001b[34mure of in the will of the general of th\u001b[m\n",
      "-----\n",
      "\u001b[32mginning of the revolution,\n",
      "flung away it\u001b[34m is all in the world of the strong of t\u001b[m\n",
      "-----\n",
      "\u001b[32mand attained through the self-evolving o\u001b[34mf the strong of the moral the man of th\u001b[m\n",
      "-----\n",
      "\u001b[32mts and ancestors in his\n",
      "constitution, wh\u001b[34mich is not the great of the soul of the\u001b[m\n",
      "-----\n",
      "\u001b[32mught, is that the individual fixes his m\u001b[34man and more preative the prestion of th\u001b[m\n",
      "-----\n",
      "\u001b[32m the two\n",
      "principal functions of man. to \u001b[34msee to see the profent of the seems of \u001b[m\n",
      "-----\n",
      "\u001b[32mrecisely by its protean\n",
      "arts that it is \u001b[34ma sention of the experious of a conscie\u001b[m\n",
      "-----\n",
      "\u001b[32ms limits, the range of man's inner exper\u001b[34mial of the fect of the serfance of the \u001b[m\n",
      "-----\n",
      "\u001b[32m equally so his regained security. such \u001b[34mthe constinct of the find of the sense \u001b[m\n",
      "-----\n",
      "\u001b[32mng long periods should regulate their\n",
      "co\u001b[34mnsequently and and desidity of the most\u001b[m\n",
      "-----\n",
      "\u001b[32monic\n",
      "nature, which understood much, and \u001b[34mthe resire of the morality of the most \u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "moment of inexplicable hesitation, like\u001b[34m the will not a man and the consequentl\u001b[m\n",
      "-----\n",
      "\u001b[32mphere--of the moral\n",
      "maxim has almost ine\u001b[34mther to the higher of the consequent an\u001b[m\n",
      "-----\n",
      "\u001b[32mg of\n",
      "the restlessness, emptiness, and no\u001b[34mt the say and all the consequently of t\u001b[m\n",
      "-----\n",
      "\u001b[32m, as cook for thousands of years, have d\u001b[34mecised to the resting of the strong of \u001b[m\n",
      "-----\n",
      "\u001b[32mogical nature. yet were there\n",
      "steps affo\u001b[34mrmand and has to be our and the more st\u001b[m\n",
      "-----\n",
      "\u001b[32min\n",
      "itself,\" is perhaps the greatest auda\u001b[34mys of the morality of the fears of the \u001b[m\n",
      "-----\n",
      "\u001b[32min one breath. such periods as occur in \u001b[34mthe most such and the man of the strong\u001b[m\n",
      "-----\n",
      "\u001b[32meen partially attempted in the following\u001b[34m of the consequent and cannot of the co\u001b[m\n",
      "-----\n",
      "\u001b[32mcount, as instruments, they are\n",
      "far from\u001b[34m the strong of the seems of the sense o\u001b[m\n",
      "-----\n",
      "\u001b[32mand calls itself good, is the instinct o\u001b[34mf the interpression of the most strange\u001b[m\n",
      "-----\n",
      "\u001b[32mes\"; and in the end,\n",
      "in view of all that\u001b[34m is a still to the most sense of the co\u001b[m\n",
      "-----\n",
      "\u001b[32m-i wager he\n",
      "finds nothing!\n",
      "\n",
      "36. supposin\u001b[34mg the devility of the feeling of the ma\u001b[m\n",
      "-----\n",
      "\u001b[32med and attracted by the riddles which th\u001b[34me long of the presisition of the will t\u001b[m\n",
      "-----\n",
      "\u001b[32mpplied\n",
      "to actions; it is a gross mistake\u001b[34m to be inderitions of the forment of th\u001b[m\n",
      "-----\n",
      "\u001b[32mn fell,\n",
      "     and unlearned man and god a\u001b[34mnd the sense of the same of the servent\u001b[m\n",
      "-----\n",
      "\u001b[32m the constant out-looking and down-looki\u001b[34mng of the sense of the freedous of the \u001b[m\n",
      "-----\n",
      "\u001b[32m to divine and\n",
      "determine what sort of hi\u001b[34ms seems to see and a precience of the m\u001b[m\n",
      "-----\n",
      "\u001b[32mnew ethical adjustment, it is then decid\u001b[34me the most decise of the man of the sam\u001b[m\n",
      "-----\n",
      "\u001b[32mabout love:\n",
      "the martyrdom of the most in\u001b[34m the old formance of the man of the man\u001b[m\n",
      "-----\n",
      "\u001b[32m by a distinguished logician, will be il\u001b[34ml the most still of the former of the p\u001b[m\n",
      "-----\n",
      "\u001b[32me of the subtler after-effects of democr\u001b[34ming and something of the sense of the h\u001b[m\n",
      "-----\n",
      "\u001b[32med by the super-abundance of power. the\n",
      "\u001b[34mdesines of the same of the desist of th\u001b[m\n",
      "-----\n",
      "\u001b[32m worse.\n",
      "\n",
      "[5] pneumatic is here used in t\u001b[34mhe sense of the \"good of the free as a \u001b[m\n",
      "-----\n",
      "\u001b[32me strength of a mind might be measured b\u001b[34me desistion of the intertain of the res\u001b[m\n",
      "-----\n",
      "\u001b[32mteems, but only when one\n",
      "esteems equal o\u001b[34mf the morality of the stranger of the m\u001b[m\n",
      "-----\n",
      "\u001b[32morigin and history of the so-called mora\u001b[34ml man the strong of the fect of the say\u001b[m\n",
      "-----\n",
      "\u001b[32mity in wrong--i mean to\n",
      "say into general\u001b[34m the most stroent of the sense of the m\u001b[m\n",
      "-----\n",
      "\u001b[32mly under them. therefore: in sleep and i\u001b[34mn the sense of the soul of the morality\u001b[m\n",
      "-----\n",
      "\u001b[32mey evidently appear too rarely, they are\u001b[34m not the stould and the desting of the \u001b[m\n",
      "-----\n",
      "\u001b[32mn treat it with moral\n",
      "indignation. enoug\u001b[34mh the history of the freedom of the sta\u001b[m\n",
      "-----\n",
      "\u001b[32me in himself\" the objective man is in tr\u001b[34messelves of the man of the stranger of \u001b[m\n",
      "-----\n",
      "\u001b[32mthey are overturned. it is wilfulness an\u001b[34md consequently and freedoping the conse\u001b[m\n",
      "-----\n",
      "\u001b[32m.\n",
      "\n",
      "\n",
      "\n",
      "chapter iv. apophthegms and interlu\u001b[34mse of such and all the most still of th\u001b[m\n",
      "-----\n",
      "\u001b[32mesitation, protractedness, frequent retr\u001b[34messive of the sense of the strong of th\u001b[m\n",
      "-----\n",
      "\u001b[32mt the older psychologists had a\n",
      "merrier \u001b[34mand the sense of the will the consequen\u001b[m\n",
      "-----\n",
      "\u001b[32m of a merely\n",
      "moral man\"--it would make t\u001b[34mhe constinct and the intellection of th\u001b[m\n",
      "-----\n",
      "\u001b[32me belief that there are\n",
      "like things (gle\u001b[34mat the stroegh of the his of the freedo\u001b[m\n",
      "-----\n",
      "\u001b[32morigin,\n",
      "with regard to the inherited ple\u001b[34mated and not be the man of every strong\u001b[m\n",
      "-----\n",
      "\u001b[32mances to induce perplexity in the mind, \u001b[34mwhich have be the expection of the most\u001b[m\n",
      "-----\n",
      "\u001b[32mr generation in the future. for metaphys\u001b[34mical and in the spirit of the stroegh o\u001b[m\n",
      "-----\n",
      "\u001b[32mfficult for\n",
      "a noble man to understand: h\u001b[34me is the expection of the expestion of \u001b[m\n",
      "-----\n",
      "\u001b[32mtes, upsets the table, shrieks, raves,\n",
      "a\u001b[34mnd the devely of the man of the strong \u001b[m\n",
      "-----\n",
      "\u001b[32mng; for instance,\n",
      "virtue, art, music, da\u001b[34msing the stranger of the presience of a\u001b[m\n",
      "-----\n",
      "\u001b[32m\"\n",
      "\n",
      "210. supposing, then, that in the pic\u001b[34mt of the strong of the more of the sens\u001b[m\n",
      "-----\n",
      "\u001b[32mor me, i\n",
      "now love every fate:--who would\u001b[34m and the strong the man of the stands o\u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "epochs, when they show themselves infec\u001b[34mtions and desire of the most the sense \u001b[m\n",
      "-----\n",
      "\u001b[32many liberties: i said\n",
      "this once before b\u001b[34meen the sense of a pression of the expe\u001b[m\n",
      "-----\n",
      "\u001b[32mas not easily a right to it\"--such a phi\u001b[34mlosopher to a seems of the morality of \u001b[m\n",
      "-----\n",
      "\u001b[32mthe dangerous dyspepsia which originates\u001b[34m the consequent of the most of more fre\u001b[m\n",
      "-----\n",
      "\u001b[32m79. the consequences of our actions seiz\u001b[34med and the presiness of the supertion o\u001b[m\n",
      "-----\n",
      "\u001b[32mneuma being the\n",
      "greek word in the new te\u001b[34mat of the expestion of the more sense o\u001b[m\n",
      "-----\n",
      "\u001b[32mensations,\n",
      "namely, the sensation of the \u001b[34mgermans of the more of the posting of t\u001b[m\n",
      "-----\n",
      "\u001b[32mand acts upon other organs thereby. the \u001b[34msense of the present of the same of the\u001b[m\n",
      "-----\n",
      "\u001b[32mwhile it delights--what? and all that is\u001b[34m a truet the restrutic and strong of th\u001b[m\n",
      "-----\n",
      "\u001b[32mfor the morrow and the day after the mor\u001b[34mal sense of the more of the experions o\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "\u001b[32mich, as we have said, france has not gru\u001b[34msses the present of the sense of the st\u001b[m\n",
      "-----\n",
      "\u001b[32mheer exhaustion. yet, in dreams, we all\n",
      "\u001b[34mthe man of the consequently of the more\u001b[m\n",
      "-----\n",
      "\u001b[32mst mind cannot\n",
      "adequately appreciate the\u001b[34m seems of the sense of the sertainty of\u001b[m\n",
      "-----\n",
      "\u001b[32ms of\n",
      "sentiment;--on this point we must n\u001b[34mot a consequently the most the experiou\u001b[m\n",
      "-----\n",
      "\u001b[32mgin in a false explanation of certain hu\u001b[34mman the stanner of the sense of the man\u001b[m\n",
      "-----\n",
      "\u001b[32me lights and colors because the brain, w\u001b[34mhich a see and the more of the will of \u001b[m\n",
      "-----\n",
      "\u001b[32muperficiality with regard to the\n",
      "origin \u001b[34mof the desting of the sense of the \"gow\u001b[m\n",
      "-----\n",
      "\u001b[32mre simply sublimations in which the basi\u001b[34mon of the morality of the man and the s\u001b[m\n",
      "-----\n",
      "\u001b[32mdequately through his\n",
      "religions and arts\u001b[34m the more of the man of the sense of th\u001b[m\n",
      "-----\n",
      "\u001b[32mythms without dance, which germans call\n",
      "\u001b[34mof the consequently of sense of the mos\u001b[m\n",
      "-----\n",
      "\u001b[32md commanding, of keeping down and keepin\u001b[34mg and have and the denession of the sta\u001b[m\n",
      "-----\n",
      "\u001b[32mde--namely,\n",
      "intellect. one of the subtle\u001b[34m of the strong of the morality of the d\u001b[m\n",
      "-----\n",
      "\u001b[32msity for training the feelings to severi\u001b[34mty of the experies of the respection of\u001b[m\n",
      "-----\n",
      "\u001b[32monditions precedent, as thus\n",
      "conjectured\u001b[34m and men and the will the consequence o\u001b[m\n",
      "-----\n",
      "\u001b[32mring\n",
      "to light! woman has so much cause f\u001b[34mor the more of the sense of the his of \u001b[m\n",
      "-----\n",
      "\u001b[32mture--when such a man has sympathy, well\u001b[34m in the strogation of the most the cons\u001b[m\n",
      "-----\n",
      "\u001b[32mlax, of course, with consideration, and\n",
      "\u001b[34msuch of the will of the sense of the se\u001b[m\n",
      "-----\n",
      "\u001b[32m thing as moral phenomena, but only a mo\u001b[34mre and strangems of the great of the mo\u001b[m\n",
      "-----\n",
      "\u001b[32mhe god, devil,\n",
      "sheep, and worm in us, in\u001b[34m the experious of the sense of the most\u001b[m\n",
      "-----\n",
      "\u001b[32mpathy for you!--to be sure, that is not \u001b[34mthe most such and something the most so\u001b[m\n",
      "-----\n",
      "\u001b[32m\n",
      "how comes it that the mind of the dream\u001b[34ms of the morality of the morality of su\u001b[m\n",
      "-----\n",
      "\u001b[32mn if the question whether anything metap\u001b[34mhysical and the most strong of the stro\u001b[m\n",
      "-----\n",
      "\u001b[32mrejudiced and almost\n",
      "hostile, is precise\u001b[34md the prestion of the seem of the same \u001b[m\n",
      "-----\n",
      "\u001b[32mts, they take control of the needy as a\n",
      "\u001b[34mstrong of the still of the restration o\u001b[m\n",
      "-----\n",
      "\u001b[32mhat\n",
      "psychological observation is one of \u001b[34mthe supperious of the morality of the m\u001b[m\n",
      "-----\n",
      "\u001b[32mer the raffinements of\n",
      "decadence--which,\u001b[34m and all the postion of the sense of th\u001b[m\n",
      "-----\n",
      "\u001b[32mhending subject\n",
      "consists in the inner ne\u001b[34mture to a servent and profound and the \u001b[m\n",
      "-----\n",
      "\u001b[32m is, indeed, still in\n",
      "process of evoluti\u001b[34mon of the sense of the fect of the most\u001b[m\n",
      "-----\n",
      "\u001b[32mre,\n",
      "a dissatisfaction with its own condi\u001b[34mtions of the morality of the most stron\u001b[m\n",
      "-----\n",
      "\u001b[32m, necessarily doubts the\n",
      "value of life; \u001b[34min the sempation of the morality of the\u001b[m\n",
      "-----\n",
      "\u001b[32m individual, into the neighbour and\n",
      "frie\u001b[34mnds of the experions of the pood of the\u001b[m\n",
      "-----\n",
      "\u001b[32mresumable that the objects of the\n",
      "religi\u001b[34mons of the sumpless of the depined of t\u001b[m\n",
      "-----\n",
      "\u001b[32mficultly\n",
      "understood\" myself!)--and one s\u001b[34meems the consection of the stands of th\u001b[m\n",
      "-----\n",
      "\u001b[32min the same way as disrespectfulness to\n",
      "\u001b[34mthe strong of the spirit of the sense o\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# For a more complete inspection, print out a load of sentences:\n",
    "#\n",
    "num_sentences = 100             # how many to generate\n",
    "sentence_length = 40            # 100--400 is good\n",
    "sample_temperature = 0.25       # see discussion of temperature up near the top\n",
    "#sample_temperature = 0.1\n",
    "\n",
    "start_index_list = np.random.randint(len(text) - maxlen - 1, size=(1, num_sentences)).flatten().tolist()\n",
    "preview_seeds = [] \n",
    "for start_index in start_index_list:\n",
    "    preview_seeds.append(text[start_index: start_index + maxlen])\n",
    "\n",
    "gru_generated_sentences = generate_sentence_list_n(preview_seeds, length=sentence_length, temperature=sample_temperature); \n",
    "print_sentences(preview_seeds, gru_generated_sentences)\n",
    "\n",
    "lstm_generated_sentences = generate_sentence_list_n(preview_seeds, length=sentence_length, temperature=sample_temperature); \n",
    "print_sentences(preview_seeds, lstm_generated_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a checkpoint, which will let you download and re-upload (or add to git) this model.\n",
    "save_model(gru_generator_model, './gru_generator_model.h5')\n",
    "save_model(lstm_generator_model, './lstm_generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating lstm sentences: [####################] 100.0% - done\n"
     ]
    }
   ],
   "source": [
    "# Generating the training fake sentences for the Discriminator network\n",
    "#\n",
    "# These are saved to the file 'fake.pkl' -- you could download this to your\n",
    "# user drive and re-upload it in a subsequent session, to save regenerating\n",
    "# it again (in which case you don't need to evaluate this cell).\n",
    "\n",
    "#training_seeds = pick_sentences(3000, maxlen=40)\n",
    "training_seeds = pick_sentences(3000, maxlen=40)\n",
    "#training_generated_sentences = (generate_sentence_list_n(training_seeds, length=40) \n",
    "#                                + generate_sentence_list_n(training_seeds, length=40, model_type='lstm'))\n",
    "training_generated_sentences = generate_sentence_list(training_seeds, length=40, temperature=0.1, model_type='lstm')\n",
    "# Strip out the initial 40 chars (the seed sequence, which is genuine data from the corpus).\n",
    "for i, sentence in enumerate(training_generated_sentences):\n",
    "    training_generated_sentences[i] = sentence[40:40+40]\n",
    "    \n",
    "output = open('fake.pkl', 'wb')\n",
    "pickle.dump(training_seeds, output)\n",
    "pickle.dump(training_generated_sentences, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set from the file\n",
    "pkl_file = open('fake.pkl', 'rb')\n",
    "training_seeds = pickle.load(pkl_file)\n",
    "training_generated_sentences = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num generated:  3000\n",
      "All training sequences:  6000\n"
     ]
    }
   ],
   "source": [
    "# Make a 50:50 set of 'fake' (generated) and genuine sentences:\n",
    "num_generated = len(training_generated_sentences)\n",
    "print('Num generated: ', num_generated)\n",
    "training_real_sentences = pick_sentences(num_generated, maxlen=40)\n",
    "\n",
    "all_training_sentences = training_generated_sentences + training_real_sentences\n",
    "\n",
    "n = len(all_training_sentences)\n",
    "x = np.zeros((n, 40, len(chars)))\n",
    "y = np.zeros((n, 1))\n",
    "print('All training sequences: ', n)\n",
    "\n",
    "for i, sentence in enumerate(all_training_sentences):\n",
    "    x[i, :, :] = onehot_encode(sentence, maxlen=40)\n",
    "y[num_generated:] = 1  # Encodes the fact that sentences with indexes larger than (num_generated) are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLossAccuracy(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.acc = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_acc = []\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(int(self.i))\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "        \n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.plot([1, 2])\n",
    "        plt.subplot(121) \n",
    "        plt.plot(self.x, self.losses, label=\"train loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"validation loss\")\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title('Model Loss')\n",
    "        plt.legend()\n",
    "        plt.subplot(122)         \n",
    "        plt.plot(self.x, self.acc, label=\"training accuracy\")\n",
    "        plt.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
    "        plt.legend()\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "compiled.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_9 (GRU)                  (None, 128)               71424     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 57)                7353      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 79,347\n",
      "Trainable params: 79,091\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "# Define some layers here..\n",
    "\n",
    "# Use your layers to create the model.\n",
    "discriminator_model = Sequential()\n",
    "#discriminator_model.add(LSTM(128, dropout=0.6, recurrent_dropout=0.7,\n",
    "#                            input_shape=(maxlen, len(chars))))\n",
    "#discriminator_model.add(LSTM(256, dropout=0.0, recurrent_dropout=0.01,\n",
    "#                             kernel_regularizer=l2(0.2),\n",
    "#                             input_shape=(maxlen, len(chars))))\n",
    "\n",
    "#inputs = Input(shape=(40, 59))\n",
    "#h = LSTM(256)(inputs)\n",
    "#h = Dropout(0.2)(h)\n",
    "#h = Dense(1024, activation='relu')(h)\n",
    "#h = LSTM(256, return_sequences=False)(h)\n",
    "#h = Dense(512, activation='tanh')(h)\n",
    "\n",
    "#output = Dense(1, activation='softmax')(h)\n",
    "\n",
    "discriminator_model = Sequential()\n",
    "#discriminator_model.add(Embedding(4020, 40, input_length=59))\n",
    "#discriminator_model.add(LSTM(128, dropout = 0.2, return_sequences=False, \n",
    "#                             recurrent_dropout = 0.2, input_shape=(maxlen, len(chars))))\n",
    "                             #unit_forget_bias=True, kernel_regularizer=l1(0.01), recurrent_regularizer=l2(0.01)))\n",
    "discriminator_model.add(GRU(128, input_shape=(maxlen, len(chars))))\n",
    "discriminator_model.add(BatchNormalization())\n",
    "#discriminator_model.add(LSTM(256, recurrent_dropout=0.0, return_sequences=False, input_shape=(maxlen, len(chars))))\n",
    "#discriminator_model.add(LSTM(128, return_sequences=True))\n",
    "#discriminator_model.add(Conv1D(64, 5, activation='relu', padding='valid', input_shape=(maxlen, len(chars))))\n",
    "#discriminator_model.add(Conv1D(32, 3, activation='tanh', padding='same'))\n",
    "#discriminator_model.add(Dropout(0.7))\n",
    "#discriminator_model.add(Dense(len(chars), activation='relu'))\n",
    "#discriminator_model.add(LSTM(128, return_sequences=False))\n",
    "#discriminator_model.add(Flatten())\n",
    "#discriminator_model.add(Dropout(0.7))\n",
    "#discriminator_model.add(LSTM(256, dropout = 0.5, return_sequences=True, recurrent_dropout = 0.2))\n",
    "#discriminator_model.add(GRU(64))\n",
    "#discriminator_model.add(Flatten())\n",
    "discriminator_model.add(Dense(len(chars), activation='softmax'))\n",
    "#discriminator_model.add(Dense(1, activation='sigmoid'))\n",
    "#discriminator_model.add(Dense(1024))\n",
    "#discriminator_model.add(LeakyReLU(0.2))\n",
    "#discriminator_model.add(Dense(512))\n",
    "#discriminator_model.add(LeakyReLU(0.2))\n",
    "#discriminator_model.add(Dropout(0.4))\n",
    "#discriminator_model.add(Dense(1))\n",
    "\n",
    "discriminator_model.add(Dense(1, activation='sigmoid'))\n",
    "opt = RMSprop(lr=0.01)\n",
    "\n",
    "# Setup the optimisation strategy.\n",
    "discriminator_model.compile(optimizer='nadam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "                             \n",
    "print('compiled.')\n",
    "discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "4500/4500 [==============================] - 7s 2ms/step - loss: 0.3016 - acc: 0.9924 - val_loss: 0.2856 - val_acc: 0.9947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227c1a3e518>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_train, x_test, y_train, y_test] = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "discriminator_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=1, batch_size=64)\n",
    "#pltCallBack = PlotLossAccuracy()\n",
    "#discriminator_model.fit(x_train, y_train, \n",
    "#                        validation_data=(x_test, y_test), \n",
    "#                        epochs=3, batch_size=64,\n",
    "#                        callbacks=[pltCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.29\n",
      "Training Accuracy: 99.62%\n",
      "Validation Accuracy: 99.47%\n"
     ]
    }
   ],
   "source": [
    "# Once you're happy with your discriminator model, evaluate this cell to save it:\n",
    "save_model(discriminator_model, './discriminator_model.h5')\n",
    "# Run these commands in the terminal to submit your model for assessment.\n",
    "# git add lab-07/discriminator_model.h5\n",
    "# git commit -m \"Add/update discriminator model.\"\n",
    "# git push\n",
    "# submit-lab 7\n",
    "\n",
    "score,train = discriminator_model.evaluate(x_train, y_train, batch_size = 64, verbose=0)\n",
    "score,acc = discriminator_model.evaluate(x_test, y_test, batch_size = 64, verbose=0)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Training Accuracy: %.2f%%\" % (train*100))\n",
    "print(\"Validation Accuracy: %.2f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
